{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploration of *Amorpha* spp. GBIF distributions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import libraries and set directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Import libraries\n",
    "import pygbif.occurrences as occ\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patheffects import withStroke # For text effects\n",
    "import matplotlib.cm as cm # For colormaps\n",
    "import numpy as np\n",
    "import time # adds delay between gbif api calls\n",
    "from shapely.geometry import box # To create a bounding box\n",
    "import rasterio # For raster data\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show as raster_show # For easier plotting with transform\n",
    "from rasterio.sample import sample_gen # For extracting raster values at points\n",
    "import seaborn as sns # For a nicer heatmap visualization\n",
    "from scipy.stats import pearsonr # For R-squared calculation (pearsonr returns r, so r^2)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Data directories\n",
    "# area of interest\n",
    "aoi_dir = r'data\\aoi'\n",
    "# GBIF observations\n",
    "obs_dir = r'data\\species'\n",
    "# Bioclim variables\n",
    "bio_vars_dir = r'data/bioclim/wc2.1_2.5m_bio'\n",
    "# Elevation raster\n",
    "elev_file_path = r'data/bioclim/wc2.1_2.5m_elev/wc2.1_2.5m_elev.tif'\n",
    "\n",
    "# Say when you're ready!\n",
    "print(f\"Current Working Directory: {os.getcwd()}\") # Good to check!\n",
    "print(f\"AOI directory will be: {os.path.abspath(aoi_dir)}\")\n",
    "print(f\"Observations directory will be: {os.path.abspath(obs_dir)}\")\n",
    "\n",
    "print(\"Libraries imported!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read the species list from the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Read the species list from the CSV ---\n",
    "filename = 'species_gbif.csv'\n",
    "species_csv_path = os.path.join(obs_dir, filename)\n",
    "try:\n",
    "    df_species_list = pd.read_csv(species_csv_path)\n",
    "    if 'simpSciName' not in df_species_list.columns:\n",
    "        print(f\"Error: Column 'species' not found in {species_csv_path}. Please check the CSV.\")\n",
    "        species_list = []\n",
    "    else:\n",
    "        # Ensure we only get actual species names, not the header if it was misread.\n",
    "        # Filter out any row where 'species' column might be the header 'species' itself.\n",
    "        df_species_list_filtered = df_species_list[df_species_list['simpSciName'] != 'simpSciName']\n",
    "        species_list = df_species_list_filtered['simpSciName'].dropna().unique().tolist()\n",
    "    \n",
    "    if not species_list: # If list is empty after filtering\n",
    "        print(f\"No valid species names found in {species_csv_path} after filtering. Please check the CSV content.\")\n",
    "    else:\n",
    "        print(f\"Found {len(species_list)} unique species in the CSV: {species_list}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Species CSV file not found at '{species_csv_path}'.\")\n",
    "    species_list = []\n",
    "except Exception as e:\n",
    "    print(f\"Error reading species CSV file '{species_csv_path}': {e}\")\n",
    "    species_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: GBIF API Query\n",
    "GBIF occurence data will either be loaded in from a previous query csv or queried fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Fetch or Load Occurrence Data ---\n",
    "output_csv_filename = 'df_all_occurrences.csv'\n",
    "output_csv_path = os.path.join(obs_dir, output_csv_filename)\n",
    "\n",
    "# Check if the processed CSV file already exists\n",
    "if os.path.exists(output_csv_path):\n",
    "    print(f\"Found existing processed data file: '{output_csv_path}'. Loading data from CSV...\")\n",
    "    try:\n",
    "        df_all_occurrences = pd.read_csv(output_csv_path)\n",
    "        print(f\"Successfully loaded {len(df_all_occurrences)} records from CSV.\")\n",
    "        if not df_all_occurrences.empty:\n",
    "            print(\"Sample of loaded data (first 5 rows):\")\n",
    "            # Display some relevant columns, adjust as needed\n",
    "            display_cols = ['scientificName', 'decimalLatitude', 'decimalLongitude', 'key', 'issues']\n",
    "            existing_cols = [col for col in display_cols if col in df_all_occurrences.columns]\n",
    "            print(df_all_occurrences[existing_cols].head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from '{output_csv_path}': {e}\")\n",
    "        print(\"Proceeding to fetch data from GBIF.\")\n",
    "        df_all_occurrences = pd.DataFrame() # Ensure it's an empty DataFrame\n",
    "else:\n",
    "    print(f\"Processed data file not found at '{output_csv_path}'. Fetching data from GBIF...\")\n",
    "    all_occurrences_list = []\n",
    "    \n",
    "    # Ensure species_list is defined from Step 1\n",
    "    # If not, you might need to load it or define it here. For example:\n",
    "    # species_list = [...] # or load from species_gbif.csv as in your Step 1\n",
    "\n",
    "    if not 'species_list' in locals() or not species_list:\n",
    "        print(\"Error: 'species_list' is not defined or is empty. Cannot fetch GBIF data.\")\n",
    "        print(\"Please ensure Step 1 (reading species list) has been executed successfully.\")\n",
    "        df_all_occurrences = pd.DataFrame() # Initialize as empty\n",
    "    else:\n",
    "        print(\"\\nFetching GBIF occurrence data for each species...\")\n",
    "        PAGE_LIMIT = 300 # Max records per page GBIF allows\n",
    "\n",
    "        for species_name in species_list:\n",
    "            if not species_name or pd.isna(species_name):\n",
    "                print(f\"Skipping invalid species name: {species_name}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nSearching for all occurrences of '{species_name}' in North America with coordinates...\")\n",
    "            \n",
    "            current_offset = 0\n",
    "            species_records = []\n",
    "            records_fetched_for_species = 0\n",
    "\n",
    "            while True:\n",
    "                params = {\n",
    "                    'scientificName': species_name,\n",
    "                    'hasCoordinate': True,\n",
    "                    'continent': 'north_america',\n",
    "                    'limit': PAGE_LIMIT,\n",
    "                    'offset': current_offset\n",
    "                }\n",
    "                \n",
    "                try:\n",
    "                    response = occ.search(**params)\n",
    "                    results_batch = response.get('results', [])\n",
    "                    \n",
    "                    if results_batch:\n",
    "                        species_records.extend(results_batch)\n",
    "                        records_fetched_for_species += len(results_batch)\n",
    "                        print(f\"  Fetched {len(results_batch)} records (offset {current_offset}). Total for '{species_name}': {records_fetched_for_species}\")\n",
    "                    \n",
    "                    if response.get('endOfRecords', True) or not results_batch:\n",
    "                        print(f\"  End of records for '{species_name}'. Total found: {records_fetched_for_species}\")\n",
    "                        break \n",
    "                    \n",
    "                    current_offset += PAGE_LIMIT\n",
    "                    time.sleep(0.2)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  An error occurred while fetching data for '{species_name}' (offset {current_offset}): {e}\")\n",
    "                    print(\"  Continuing to the next species or offset if possible, but this batch might be lost.\")\n",
    "                    break \n",
    "\n",
    "            if species_records:\n",
    "                df_single_species = pd.DataFrame(species_records)\n",
    "                all_occurrences_list.append(df_single_species)\n",
    "            else:\n",
    "                if records_fetched_for_species == 0:\n",
    "                     print(f\"No occurrences ultimately processed for '{species_name}' with the specified criteria after pagination attempt.\")\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "\n",
    "        # Concatenate all occurrences into a single DataFrame\n",
    "        if all_occurrences_list:\n",
    "            df_all_occurrences = pd.concat(all_occurrences_list, ignore_index=True)\n",
    "            print(f\"\\nTotal occurrences fetched for all species: {len(df_all_occurrences)}\")\n",
    "            if not df_all_occurrences.empty:\n",
    "                print(\"Sample of combined data (first 5 rows):\")\n",
    "                print(df_all_occurrences[['scientificName', 'decimalLatitude', 'decimalLongitude', 'key']].head())\n",
    "                \n",
    "                # --- Write to CSV ---\n",
    "                try:\n",
    "                    # Ensure the directory exists before writing\n",
    "                    os.makedirs(obs_dir, exist_ok=True)\n",
    "                    df_all_occurrences.to_csv(output_csv_path, index=False)\n",
    "                    print(f\"\\nSuccessfully saved all occurrences to '{output_csv_path}'\")\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError saving data to '{output_csv_path}': {e}\")\n",
    "            else:\n",
    "                print(\"\\nNo occurrences were fetched, so no CSV file was saved.\")\n",
    "        else:\n",
    "            print(\"\\nNo occurrences were fetched for any species. No CSV file was saved.\")\n",
    "            df_all_occurrences = pd.DataFrame() # Ensure it's an empty DataFrame if nothing was fetched\n",
    "\n",
    "# Final check and display if df_all_occurrences is still empty\n",
    "if df_all_occurrences.empty:\n",
    "    print(\"\\nNo occurrence data available (either not found in CSV or not fetched).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: Check on GBIF issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3a: Check for and handle GBIF issues ---\n",
    "if 'df_all_occurrences' not in globals() or df_all_occurrences.empty:\n",
    "    print(\"Error: The DataFrame 'df_all_occurrences' is not defined or is empty.\")\n",
    "    print(\"Please ensure it's populated with the concatenated GBIF data before running this analysis.\")\n",
    "else:\n",
    "    print(f\"Total records in df_all_occurrences: {len(df_all_occurrences)}\\n\")\n",
    "\n",
    "    if 'issues' not in df_all_occurrences.columns:\n",
    "        print(\"The 'issues' column was not found in the DataFrame.\")\n",
    "        print(\"This column is expected from the pygbif download and contains GBIF data quality flags.\")\n",
    "        print(f\"Available columns: {df_all_occurrences.columns.tolist()}\")\n",
    "    else:\n",
    "        print(\"--- Debugging 'issues' column ---\")\n",
    "        non_null_issues = df_all_occurrences['issues'].dropna()\n",
    "        if non_null_issues.empty:\n",
    "            print(\"Debug: The 'issues' column contains only NaN values.\")\n",
    "        else:\n",
    "            print(f\"Debug: First 5 non-null entries in 'issues' column and their types:\")\n",
    "            for i in range(min(5, len(non_null_issues))):\n",
    "                entry = non_null_issues.iloc[i]\n",
    "                print(f\"  Entry {i+1}: {entry} (Type: {type(entry)})\")\n",
    "        print(\"--- End Debugging ---\")\n",
    "\n",
    "\n",
    "        # Extract all individual issue strings from the 'issues' column\n",
    "        all_individual_issues = []\n",
    "        for index, row in df_all_occurrences.iterrows(): # Consider using .itertuples() for minor speedup if needed\n",
    "            issue_list_for_row = row['issues']\n",
    "            \n",
    "            # <<< POTENTIAL FIX AREA STARTS HERE >>>\n",
    "            # If 'issues' are strings like \"['ISSUE1', 'ISSUE2']\", convert them\n",
    "            if isinstance(issue_list_for_row, str):\n",
    "                try:\n",
    "                    # Attempt to evaluate the string as a Python literal (list)\n",
    "                    import ast\n",
    "                    evaluated_list = ast.literal_eval(issue_list_for_row)\n",
    "                    if isinstance(evaluated_list, list):\n",
    "                        issue_list_for_row = evaluated_list\n",
    "                    else:\n",
    "                        # It was a string but didn't evaluate to a list\n",
    "                        # Might be a single issue string not in list format, or something else.\n",
    "                        # For now, if it's not a list after eval, we'll skip it, \n",
    "                        # or you could decide to wrap single issue strings into a list:\n",
    "                        # if isinstance(evaluated_list, str): issue_list_for_row = [evaluated_list] else: issue_list_for_row = []\n",
    "                        pass # Keep as original string, will be skipped by 'isinstance(list)' below\n",
    "                except (ValueError, SyntaxError):\n",
    "                    # Not a string representation of a list (e.g., just \"ZERO_COORDINATE\")\n",
    "                    # If you expect single issues as plain strings, you might want to wrap them:\n",
    "                    # issue_list_for_row = [issue_list_for_row] \n",
    "                    # For now, we'll assume GBIF issues are list-like if stringified\n",
    "                    pass # Keep as original string, will be skipped \n",
    "            # <<< POTENTIAL FIX AREA ENDS HERE >>>\n",
    "\n",
    "            if isinstance(issue_list_for_row, list): # Check if it's a list\n",
    "                all_individual_issues.extend(issue_list_for_row)\n",
    "            # If issue_list_for_row is NaN or not a list (even after trying to convert string), it's skipped.\n",
    "\n",
    "        if not all_individual_issues:\n",
    "            print(\"No issues were successfully extracted into 'all_individual_issues'.\")\n",
    "            print(\"This could be due to the 'issues' column format (e.g., strings not lists, all NaNs, or unexpected structure).\")\n",
    "            print(\"Please check the debug output above for the format of entries in the 'issues' column.\")\n",
    "        else:\n",
    "            all_issues_series = pd.Series(all_individual_issues)\n",
    "            issue_counts = all_issues_series.value_counts()\n",
    "            \n",
    "            print(\"\\nCounts of records per GBIF Issue Category:\") # Added newline for better spacing\n",
    "            # ... (rest of your original code for printing counts)\n",
    "            print(\"------------------------------------------------------------------------------------\")\n",
    "            print(\"Geospatial Issues (in approximate order from the GBIF blog post):\")\n",
    "\n",
    "            ORDERED_GEOSPATIAL_ISSUES = {\n",
    "                \"ZERO_COORDINATE\": \"Zero coordinate (0,0 - often null)\",\n",
    "                \"COUNTRY_COORDINATE_MISMATCH\": \"Country coordinate mismatch\",\n",
    "                \"COORDINATE_INVALID\": \"Coordinate invalid (uninterpretable)\",\n",
    "                \"COORDINATE_OUT_OF_RANGE\": \"Coordinate out of range (-90/90, -180/180)\",\n",
    "                \"GEODETIC_DATUM_ASSUMED_WGS84\": \"Geodetic datum assumed WGS84 (Info: datum was null)\",\n",
    "                \"GEODETIC_DATUM_INVALID\": \"Geodetic datum invalid\",\n",
    "                \"COUNTRY_MISMATCH\": \"Country mismatch (interpreted name vs. code)\",\n",
    "                \"COUNTRY_DERIVED_FROM_COORDINATES\": \"Country derived from coordinates (Info: country was null)\",\n",
    "                \"COUNTRY_INVALID\": \"Country invalid (uninterpretable name/code)\",\n",
    "                \"CONTINENT_INVALID\": \"Continent invalid\",\n",
    "                \"COORDINATE_ROUNDED\": \"Coordinate rounded by GBIF (Info: standard processing)\",\n",
    "                \"COORDINATE_REPROJECTED\": \"Coordinate reprojected to WGS84 (Info: successful)\",\n",
    "                \"COORDINATE_REPROJECTION_SUSPICIOUS\": \"Coordinate reprojection suspicious (large shift)\",\n",
    "                \"COORDINATE_REPROJECTION_FAILED\": \"Coordinate reprojection failed\",\n",
    "                \"COORDINATE_UNCERTAINTY_METERS_INVALID\": \"Coordinate uncertainty meters invalid\",\n",
    "                \"COORDINATE_PRECISION_INVALID\": \"Coordinate precision invalid\",\n",
    "                \"PRESUMED_NEGATED_LONGITUDE\": \"Presumed negated longitude (to match country)\",\n",
    "                \"PRESUMED_NEGATED_LATITUDE\": \"Presumed negated latitude (to match country)\"\n",
    "            }\n",
    "            \n",
    "            issues_printed_explicitly = set()\n",
    "\n",
    "            for issue_code, description in ORDERED_GEOSPATIAL_ISSUES.items():\n",
    "                count = issue_counts.get(issue_code, 0)\n",
    "                print(f\"- {description} ({issue_code}): {count} records\")\n",
    "                issues_printed_explicitly.add(issue_code)\n",
    "\n",
    "            print(\"\\nOther Issues Found (including other categories like taxonomic, date-related, etc.):\")\n",
    "            print(\"----------------------------------------------------------------------------------\")\n",
    "            \n",
    "            other_issues_found_count = 0\n",
    "            for issue_code, count in issue_counts.items():\n",
    "                if issue_code not in issues_printed_explicitly:\n",
    "                    description = ORDERED_GEOSPATIAL_ISSUES.get(issue_code, issue_code) \n",
    "                    print(f\"- {description}: {count} records\")\n",
    "                    other_issues_found_count += 1\n",
    "            \n",
    "            if other_issues_found_count == 0:\n",
    "                print(\"No other significant issue types found beyond the primary geospatial list, or they had zero counts.\")\n",
    "                \n",
    "            records_with_any_issue = 0\n",
    "            for i_list in df_all_occurrences['issues']: # Re-check logic here based on actual format\n",
    "                processed_list = i_list\n",
    "                if isinstance(i_list, str):\n",
    "                    try:\n",
    "                        import ast\n",
    "                        evaluated = ast.literal_eval(i_list)\n",
    "                        if isinstance(evaluated, list):\n",
    "                            processed_list = evaluated\n",
    "                        else: # was a string but not a list, treat as no issue for this summary\n",
    "                            processed_list = [] \n",
    "                    except (ValueError, SyntaxError): # not a string list\n",
    "                        processed_list = [] # treat as no issue for this summary\n",
    "                \n",
    "                if isinstance(processed_list, list) and len(processed_list) > 0:\n",
    "                    records_with_any_issue +=1\n",
    "            \n",
    "            print(f\"\\nSummary:\")\n",
    "            print(f\"- Total records with at least one listed issue: {records_with_any_issue}\")\n",
    "            records_without_issues = len(df_all_occurrences) - records_with_any_issue\n",
    "            print(f\"- Total records with no listed issues: {records_without_issues}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b: Filter problematic observations\n",
    "- I think we'll still want to do some more filtering\n",
    "- Obs that have coords associated with collection institutions\n",
    "- Obs that are in water\n",
    "- Obs that are a countries centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3b: Filter out problematic records ---\n",
    "import ast # Make sure ast is imported\n",
    "\n",
    "if 'df_all_occurrences' not in globals() or df_all_occurrences.empty:\n",
    "    print(\"Error: The DataFrame 'df_all_occurrences' is not defined or is empty.\")\n",
    "    print(\"Please ensure it's populated with the concatenated GBIF data before running this filtering step.\")\n",
    "    df_all_occurrences_filtered = pd.DataFrame() \n",
    "else:\n",
    "    print(f\"\\nStarting filtering based on critical GBIF issues...\")\n",
    "    print(f\"Original number of records: {len(df_all_occurrences)}\")\n",
    "\n",
    "    CRITICAL_ISSUES_TO_FILTER = {\n",
    "        \"ZERO_COORDINATE\",\n",
    "        \"COORDINATE_INVALID\",\n",
    "        \"COORDINATE_OUT_OF_RANGE\",\n",
    "        \"COUNTRY_COORDINATE_MISMATCH\", # This was one you asked to filter by earlier\n",
    "        \"COORDINATE_REPROJECTION_FAILED\",\n",
    "        \"COORDINATE_UNCERTAINTY_METERS_INVALID\"\n",
    "    }\n",
    "    print(f\"Filtering out records with any of the following issues: {CRITICAL_ISSUES_TO_FILTER}\")\n",
    "\n",
    "    if 'issues' not in df_all_occurrences.columns:\n",
    "        print(\"Warning: 'issues' column not found in df_all_occurrences. Cannot perform issue-based filtering.\")\n",
    "        df_all_occurrences_filtered = df_all_occurrences.copy()\n",
    "    else:\n",
    "        rows_to_keep_mask = []\n",
    "        actually_had_issues_to_check = 0 # Counter for debugging\n",
    "\n",
    "        for entry in df_all_occurrences['issues']: # Iterate directly over the series\n",
    "            current_issue_list = [] # Default to an empty list (no issues)\n",
    "            \n",
    "            if isinstance(entry, str):\n",
    "                try:\n",
    "                    evaluated = ast.literal_eval(entry)\n",
    "                    if isinstance(evaluated, list):\n",
    "                        current_issue_list = evaluated\n",
    "                        actually_had_issues_to_check +=1 \n",
    "                    # If it's a string but not a list (e.g. single issue name), \n",
    "                    # ast.literal_eval might return it as a string.\n",
    "                    # For simplicity here, if not a list after eval, treat as empty for filtering.\n",
    "                    # Or, you could decide: if isinstance(evaluated, str): current_issue_list = [evaluated]\n",
    "\n",
    "                except (ValueError, SyntaxError):\n",
    "                    # String was not a valid Python literal representation of a list\n",
    "                    # print(f\"Debug: Could not parse string: {entry}\") # Optional debug\n",
    "                    pass # current_issue_list remains empty\n",
    "            elif isinstance(entry, list): # Already a list (e.g., if data not from CSV this run)\n",
    "                current_issue_list = entry\n",
    "                actually_had_issues_to_check +=1\n",
    "            # If entry is NaN (float type usually), it's not a string or list, current_issue_list remains empty.\n",
    "\n",
    "            # Now, current_issue_list is either the actual list of issues or an empty list.\n",
    "            if not CRITICAL_ISSUES_TO_FILTER.intersection(set(current_issue_list)):\n",
    "                rows_to_keep_mask.append(True)  # Keep if no critical issues found in this record's list\n",
    "            else:\n",
    "                rows_to_keep_mask.append(False) # Exclude if a critical issue was found\n",
    "\n",
    "        print(f\"Debug: Number of rows where an actual list of issues was processed: {actually_had_issues_to_check}\")\n",
    "        df_all_occurrences_filtered = df_all_occurrences[rows_to_keep_mask].copy()\n",
    "\n",
    "        num_records_removed = len(df_all_occurrences) - len(df_all_occurrences_filtered)\n",
    "        print(f\"Number of records removed due to critical issues: {num_records_removed}\")\n",
    "        print(f\"Number of records remaining after filtering: {len(df_all_occurrences_filtered)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Provide simplified scientific names based on speciesKey\n",
    "Species exception handling for *Amorpha herbacea var. floridana* due to it being considered a variety of *Amorpha herbacea* in GBIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Add/update simplified scientific names based on BNM provided list ---\n",
    "\n",
    "if 'df_all_occurrences_filtered' not in globals() or df_all_occurrences_filtered.empty:\n",
    "    print(\"Error: The DataFrame 'df_all_occurrences_filtered' is not defined or is empty.\")\n",
    "    print(\"Please ensure it's populated before attempting to add/update simplified scientific names.\")\n",
    "elif 'df_species_list' not in globals() or df_species_list.empty:\n",
    "    print(\"Error: The DataFrame 'df_species_list' (your species map) is not defined or is empty.\")\n",
    "    print(\"Please ensure Step 1 (loading species list/map) has been executed successfully.\")\n",
    "else:\n",
    "    print(f\"\\nAttempting to add/update 'simpSciName' in df_all_occurrences_filtered using the pre-loaded 'df_species_list' and specific rules...\")\n",
    "    \n",
    "    # df_species_map now refers to your already loaded df_species_list\n",
    "    df_species_map = df_species_list.copy() # Use a copy to avoid modifying the original df_species_list\n",
    "\n",
    "    # --- Part 1: Merge based on speciesKey from df_species_map (which is df_species_list) ---\n",
    "    if 'speciesKey' not in df_species_map.columns or 'simpSciName' not in df_species_map.columns:\n",
    "        print(f\"Error: The pre-loaded 'df_species_list' (used as map) must contain 'speciesKey' and 'simpSciName' columns.\")\n",
    "        print(f\"Available columns in 'df_species_list': {df_species_map.columns.tolist()}\")\n",
    "    elif 'speciesKey' not in df_all_occurrences_filtered.columns:\n",
    "        print(f\"Error: 'speciesKey' column not found in df_all_occurrences_filtered.\")\n",
    "        print(f\"Available columns in df_all_occurrences_filtered: {df_all_occurrences_filtered.columns.tolist()}\")\n",
    "    else:\n",
    "        original_occurrence_keys_type = df_all_occurrences_filtered['speciesKey'].dtype\n",
    "        original_map_keys_type = df_species_map['speciesKey'].dtype\n",
    "        \n",
    "        temp_occurrences_df = df_all_occurrences_filtered.copy()\n",
    "        # temp_map_df is already a copy (df_species_map)\n",
    "\n",
    "        try:\n",
    "            temp_occurrences_df['speciesKey_for_merge'] = pd.to_numeric(temp_occurrences_df['speciesKey'], errors='coerce').astype('Int64')\n",
    "            df_species_map['speciesKey_for_merge'] = pd.to_numeric(df_species_map['speciesKey'], errors='coerce').astype('Int64')\n",
    "            \n",
    "            temp_occurrences_df.dropna(subset=['speciesKey_for_merge'], inplace=True)\n",
    "            df_species_map.dropna(subset=['speciesKey_for_merge'], inplace=True)\n",
    "            print(f\"  Converted 'speciesKey' in occurrences data (was {original_occurrence_keys_type}) and mapping file (was {original_map_keys_type}) to 'Int64' for merging.\")\n",
    "        except Exception as e_conv:\n",
    "            print(f\"  Warning: Error during 'speciesKey' type conversion: {e_conv}. Merge might be inaccurate. Using original types.\")\n",
    "            temp_occurrences_df['speciesKey_for_merge'] = temp_occurrences_df['speciesKey']\n",
    "            df_species_map['speciesKey_for_merge'] = df_species_map['speciesKey']\n",
    "\n",
    "        original_simp_sci_name_exists = 'simpSciName' in temp_occurrences_df.columns\n",
    "        if original_simp_sci_name_exists:\n",
    "             temp_occurrences_df.rename(columns={'simpSciName': 'simpSciName_before_key_merge'}, inplace=True)\n",
    "             print(\"  Temporarily renamed existing 'simpSciName' to 'simpSciName_before_key_merge'.\")\n",
    "\n",
    "        df_all_occurrences_filtered_merged = pd.merge(\n",
    "            temp_occurrences_df,\n",
    "            df_species_map[['speciesKey_for_merge', 'simpSciName']], # Use df_species_map here\n",
    "            on='speciesKey_for_merge',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        if 'simpSciName_before_key_merge' in df_all_occurrences_filtered_merged.columns:\n",
    "            df_all_occurrences_filtered_merged['simpSciName'] = df_all_occurrences_filtered_merged['simpSciName'].fillna(\n",
    "                df_all_occurrences_filtered_merged['simpSciName_before_key_merge']\n",
    "            )\n",
    "            df_all_occurrences_filtered_merged.drop(columns=['simpSciName_before_key_merge'], inplace=True)\n",
    "        \n",
    "        df_all_occurrences_filtered_merged.drop(columns=['speciesKey_for_merge'], inplace=True, errors='ignore')\n",
    "        df_all_occurrences_filtered = df_all_occurrences_filtered_merged\n",
    "\n",
    "        num_mapped = df_all_occurrences_filtered['simpSciName'].notna().sum()\n",
    "        num_total = len(df_all_occurrences_filtered)\n",
    "        print(f\"  Merge based on speciesKey complete. 'simpSciName' column processed.\")\n",
    "        print(f\"  {num_mapped} out of {num_total} records now have a non-null 'simpSciName'.\")\n",
    "\n",
    "    # --- Part 2: Apply specific rule for 'Amorpha herbacea var. floridana (Rydb.) Wilbur' ---\n",
    "    target_scientific_name = 'Amorpha herbacea var. floridana (Rydb.) Wilbur'\n",
    "    new_simp_sci_name_for_target = 'Amorpha herbacea var. floridana'\n",
    "\n",
    "    if 'scientificName' not in df_all_occurrences_filtered.columns:\n",
    "        print(f\"\\nWarning: 'scientificName' column not found in df_all_occurrences_filtered. Cannot apply specific rule for '{target_scientific_name}'.\")\n",
    "    else:\n",
    "        mask_specific_species = (df_all_occurrences_filtered['scientificName'] == target_scientific_name)\n",
    "        num_specific_records = mask_specific_species.sum()\n",
    "\n",
    "        if num_specific_records > 0:\n",
    "            if 'simpSciName' not in df_all_occurrences_filtered.columns:\n",
    "                df_all_occurrences_filtered['simpSciName'] = pd.NA \n",
    "            df_all_occurrences_filtered.loc[mask_specific_species, 'simpSciName'] = new_simp_sci_name_for_target\n",
    "            print(f\"\\nApplied specific rule: Set 'simpSciName' to '{new_simp_sci_name_for_target}' for {num_specific_records} records where scientificName was '{target_scientific_name}'.\")\n",
    "        else:\n",
    "            print(f\"\\nSpecific rule: No records found with scientificName '{target_scientific_name}'. No changes made by this rule.\")\n",
    "\n",
    "    print(\"\\nFinal first 5 rows of df_all_occurrences_filtered with 'simpSciName' column after all processing:\")\n",
    "    if 'simpSciName' in df_all_occurrences_filtered.columns:\n",
    "        print(df_all_occurrences_filtered[['scientificName', 'speciesKey', 'simpSciName']].head())\n",
    "    else:\n",
    "        print(df_all_occurrences_filtered[['scientificName', 'speciesKey']].head(), \"(simpSciName column not present)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5/6/7: Basic plotting data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Data Cleaning (from df_all_occurrences_filtered) and Initial GeoDataFrame creation (unprojected) ---\n",
    "gdf_all_occurrences_unprojected = gpd.GeoDataFrame() # Initialize an empty GeoDataFrame\n",
    "\n",
    "if 'df_all_occurrences_filtered' not in globals() or df_all_occurrences_filtered.empty:\n",
    "    print(\"Error: df_all_occurrences_filtered is not defined or is empty. Cannot proceed with mapping preparation.\")\n",
    "else:\n",
    "    print(f\"\\nProcessing df_all_occurrences_filtered (contains {len(df_all_occurrences_filtered)} records) for mapping...\")\n",
    "    if 'decimalLatitude' in df_all_occurrences_filtered.columns and 'decimalLongitude' in df_all_occurrences_filtered.columns:\n",
    "        # Create df_all_occurrences_clean from df_all_occurrences_filtered\n",
    "        df_all_occurrences_clean = df_all_occurrences_filtered.dropna(\n",
    "            subset=['decimalLatitude', 'decimalLongitude']\n",
    "        ).copy() # .copy() is good practice\n",
    "        \n",
    "        if not df_all_occurrences_clean.empty:\n",
    "            print(f\"  {len(df_all_occurrences_clean)} records remaining after dropping rows with NA coordinates.\")\n",
    "            try:\n",
    "                gdf_all_occurrences_unprojected = gpd.GeoDataFrame(\n",
    "                    df_all_occurrences_clean,\n",
    "                    geometry=gpd.points_from_xy(\n",
    "                        df_all_occurrences_clean.decimalLongitude, \n",
    "                        df_all_occurrences_clean.decimalLatitude\n",
    "                    ),\n",
    "                    crs=\"EPSG:4326\" \n",
    "                )\n",
    "                print(\"  Successfully created gdf_all_occurrences_unprojected from cleaned, filtered data.\")\n",
    "            except Exception as e_gdf:\n",
    "                print(f\"  Error creating GeoDataFrame from cleaned data: {e_gdf}\")\n",
    "        else:\n",
    "            print(\"  DataFrame is empty after dropping NA coordinates from df_all_occurrences_filtered.\")\n",
    "    else:\n",
    "        print(\"  Error: df_all_occurrences_filtered does not have 'decimalLatitude' or 'decimalLongitude' columns.\")\n",
    "\n",
    "if gdf_all_occurrences_unprojected.empty:\n",
    "     print(\"  gdf_all_occurrences_unprojected is empty. Further mapping steps might not produce output.\")\n",
    "\n",
    "\n",
    "# --- Step 6: Prepare Base Map (North America, Orthographic Projection) ---\n",
    "# This part remains largely the same, as it prepares the background map independently \n",
    "# of the occurrence data, other than needing to be projected to the same target CRS.\n",
    "north_america_map_proj = None\n",
    "ortho_crs = \"+proj=ortho +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +type=crs\"\n",
    "world_shapefile_path = 'data/AOI/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp'\n",
    "\n",
    "try:\n",
    "    print(f\"\\nLoading base map from '{world_shapefile_path}'...\")\n",
    "    world = gpd.read_file(world_shapefile_path)\n",
    "    if world.crs is None:\n",
    "        print(\"  Warning: Shapefile has no CRS defined. Assuming EPSG:4326 (WGS84).\")\n",
    "        world = world.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "\n",
    "    country_name_column = None\n",
    "    possible_name_cols = ['NAME', 'ADMIN', 'SOVEREIGNT', 'SOV_A3', 'ADM0_A3', 'NAME_EN', 'NAME_LONG', 'admin', 'name_long', 'name']\n",
    "    for col in possible_name_cols:\n",
    "        if col in world.columns:\n",
    "            country_name_column = col\n",
    "            break\n",
    "    \n",
    "    north_america_map_unprojected = world # Default to whole world if filtering fails\n",
    "    if country_name_column:\n",
    "        print(f\"  Using column '{country_name_column}' from shapefile for country names for base map.\")\n",
    "        north_america_countries_filter = ['United States of America', 'Canada', 'Mexico', 'United States']\n",
    "        filtered_map = world[world[country_name_column].isin(north_america_countries_filter)]\n",
    "        if not filtered_map.empty:\n",
    "            north_america_map_unprojected = filtered_map\n",
    "            print(f\"  Filtered base map to {len(north_america_map_unprojected)} North American polygons.\")\n",
    "        else:\n",
    "            print(f\"  Warning: No countries matched the filter for base map. Using all {len(world)} countries from shapefile.\")\n",
    "    else:\n",
    "        print(\"  Warning: Could not find a standard country name column for base map. Using all countries from shapefile.\")\n",
    "\n",
    "    if north_america_map_unprojected is not None and not north_america_map_unprojected.empty and north_america_map_unprojected.crs:\n",
    "        north_america_map_proj = north_america_map_unprojected.to_crs(ortho_crs)\n",
    "        print(f\"  Base map projected successfully to Orthographic CRS (centered at lat {ortho_crs.split('lat_0=')[1].split(' ')[0]}, lon {ortho_crs.split('lon_0=')[1].split(' ')[0]}).\")\n",
    "    elif north_america_map_unprojected is None or north_america_map_unprojected.empty:\n",
    "        print(\"  Could not project base map because the unprojected map is empty or None.\")\n",
    "    else: # north_america_map_unprojected exists but has no CRS\n",
    "         print(\"  Could not project base map because it has no CRS defined.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing base map shapefile: {e}\")\n",
    "\n",
    "\n",
    "# --- Step 7: Generate Species Colors (using 'simpSciName' from gdf_all_occurrences_unprojected) ---\n",
    "unique_species_in_data = [] # Initialize\n",
    "species_colors = {} # Initialize\n",
    "\n",
    "if not gdf_all_occurrences_unprojected.empty:\n",
    "    if 'simpSciName' in gdf_all_occurrences_unprojected.columns:\n",
    "        unique_species_in_data = gdf_all_occurrences_unprojected['simpSciName'].dropna().unique().tolist()\n",
    "        print(f\"\\nFound {len(unique_species_in_data)} unique 'simpSciName' values in the data for color generation.\")\n",
    "        \n",
    "        if unique_species_in_data:\n",
    "            num_unique_species = len(unique_species_in_data)\n",
    "            # Using 'tab20' colormap. For more species, colors might repeat.\n",
    "            # Consider 'viridis' or other perceptually uniform colormaps if distinctness for many categories is key.\n",
    "            if num_unique_species <= 20:\n",
    "                colors_cmap = cm.get_cmap('tab20', num_unique_species)\n",
    "                species_colors = {species: colors_cmap(i) for i, species in enumerate(unique_species_in_data)}\n",
    "            else: \n",
    "                print(f\"  Warning: More than 20 unique simplified species names ({num_unique_species}). Colors will repeat from 'tab20'.\")\n",
    "                tab20_colors_cmap = cm.get_cmap('tab20', 20) # Get 20 distinct colors\n",
    "                species_colors = {species: tab20_colors_cmap(i % 20) for i, species in enumerate(unique_species_in_data)}\n",
    "            print(f\"  Generated colors for {len(species_colors)} simplified species names.\")\n",
    "        else:\n",
    "            print(\"  No unique 'simpSciName' values found after dropping NA, so no species colors generated.\")\n",
    "    else:\n",
    "        print(\"  'simpSciName' column not found in gdf_all_occurrences_unprojected. Cannot generate species-specific colors based on it.\")\n",
    "        print(\"  Consider using 'scientificName' or another column if 'simpSciName' was not added correctly.\")\n",
    "else:\n",
    "    print(\"\\ngdf_all_occurrences_unprojected is empty. Cannot generate species colors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Plot individual species distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 9: Plot individual maps ---\n",
    "\n",
    "# 1. Occurrence Data (gdf_all_occurrences_unprojected in EPSG:4326)\n",
    "if 'gdf_all_occurrences_unprojected' not in globals() or \\\n",
    "   ('empty' in dir(gdf_all_occurrences_unprojected) and gdf_all_occurrences_unprojected.empty):\n",
    "    print(\"Setup: 'gdf_all_occurrences_unprojected' not found/empty. Creating placeholder.\")\n",
    "    placeholder_occurrence_data = pd.DataFrame({\n",
    "        'decimalLatitude': [34.0, 36.5, 38.0, 33.5, 40.0, 42.5, 28.0, 30.0, 35.0], \n",
    "        'decimalLongitude': [-118.2, -120.0, -119.5, -117.0, -75.0, -73.0, -82.0, -81.0, -95.0],\n",
    "        'simpSciName': ['Amorpha californica', 'Amorpha californica', 'Amorpha californica', 'Amorpha californica', \n",
    "                        'Species B (East)', 'Species B (East)', 'Species C (South)', 'Species C (South)', 'Species D (Central)'],\n",
    "        'key': range(9)\n",
    "    })\n",
    "    gdf_all_occurrences_unprojected = gpd.GeoDataFrame(\n",
    "        placeholder_occurrence_data,\n",
    "        geometry=gpd.points_from_xy(placeholder_occurrence_data.decimalLongitude, \n",
    "                                     placeholder_occurrence_data.decimalLatitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Setup: Using existing 'gdf_all_occurrences_unprojected'.\")\n",
    "    if not isinstance(gdf_all_occurrences_unprojected, gpd.GeoDataFrame):\n",
    "        try:\n",
    "            gdf_all_occurrences_unprojected = gpd.GeoDataFrame(\n",
    "                gdf_all_occurrences_unprojected,\n",
    "                geometry=gpd.points_from_xy(gdf_all_occurrences_unprojected.decimalLongitude, gdf_all_occurrences_unprojected.decimalLatitude),\n",
    "                crs=\"EPSG:4326\")\n",
    "        except Exception as e: print(f\"Error converting to GDF: {e}\")\n",
    "    elif gdf_all_occurrences_unprojected.crs is None or gdf_all_occurrences_unprojected.crs.to_string() != \"EPSG:4326\":\n",
    "        gdf_all_occurrences_unprojected = gdf_all_occurrences_unprojected.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# 2. North America Basemap (north_america_map_unprojected in EPSG:4326)\n",
    "if 'north_america_map_unprojected' not in globals() or \\\n",
    "   north_america_map_unprojected is None or \\\n",
    "   ('empty' in dir(north_america_map_unprojected) and north_america_map_unprojected.empty):\n",
    "    print(\"Setup: 'north_america_map_unprojected' not found/empty.\")\n",
    "    shapefile_path_actual = 'data/AOI/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp' \n",
    "    try:\n",
    "        if os.path.exists(shapefile_path_actual):\n",
    "            world_temp = gpd.read_file(shapefile_path_actual)\n",
    "            if world_temp.crs is None: world_temp = world_temp.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "            north_america_map_unprojected = world_temp[world_temp['ADMIN'].isin(['United States of America', 'Canada', 'Mexico'])]\n",
    "            if north_america_map_unprojected.empty: \n",
    "                 north_america_map_unprojected = world_temp[world_temp['ADMIN'].isin(['United States', 'Canada', 'Mexico'])] # Try alternate name\n",
    "            if north_america_map_unprojected.empty: \n",
    "                north_america_map_unprojected = gpd.GeoDataFrame({'name': ['Placeholder NA Base Map']}, geometry=[box(-179, 10, -50, 85)], crs=\"EPSG:4326\")\n",
    "            else: north_america_map_unprojected = north_america_map_unprojected.to_crs(\"EPSG:4326\")\n",
    "        else: raise FileNotFoundError \n",
    "    except Exception as e:\n",
    "        print(f\"Setup Error for basemap: {e}. Using placeholder.\")\n",
    "        north_america_map_unprojected = gpd.GeoDataFrame({'name': ['Placeholder NA Base Map']}, geometry=[box(-179, 10, -50, 85)], crs=\"EPSG:4326\")\n",
    "else:\n",
    "    print(\"Setup: Using existing 'north_america_map_unprojected'.\")\n",
    "    if north_america_map_unprojected.crs is None or north_america_map_unprojected.crs.to_string() != \"EPSG:4326\":\n",
    "        north_america_map_unprojected = north_america_map_unprojected.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# 3. Unique Species List & 4. Colors\n",
    "if 'unique_species_in_data' not in globals() or not unique_species_in_data:\n",
    "    if not gdf_all_occurrences_unprojected.empty and 'simpSciName' in gdf_all_occurrences_unprojected.columns:\n",
    "        unique_species_in_data = sorted(list(gdf_all_occurrences_unprojected['simpSciName'].dropna().unique()))\n",
    "    else: unique_species_in_data = ['Amorpha californica', 'Species B (East)', 'Species C (South)', 'Species D (Central)'] \n",
    "if 'species_colors' not in globals() or not species_colors or not all(s in species_colors for s in unique_species_in_data):\n",
    "    try: # Use seaborn for nicer palettes if available\n",
    "        import seaborn as sns; palette_gen = sns.color_palette(\"husl\", len(unique_species_in_data) if unique_species_in_data else 1)\n",
    "    except ImportError: cmap_gen = plt.cm.get_cmap('tab10', len(unique_species_in_data) if unique_species_in_data else 1); palette_gen = [cmap_gen(i) for i in range(len(unique_species_in_data) if unique_species_in_data else 1)]\n",
    "    species_colors = {name: palette_gen[i] for i, name in enumerate(unique_species_in_data)}\n",
    "# --- End of Setup ---\n",
    "\n",
    "\n",
    "# --- Determine Overall Map Extent for \"Flat\" Maps (EPSG:4326 degrees) ---\n",
    "overall_map_lim_min_lon = -125.0 \n",
    "overall_map_lim_max_lon = -65.0\n",
    "overall_map_lim_min_lat = 20.0\n",
    "overall_map_lim_max_lat = 55.0 # Default values\n",
    "\n",
    "if not gdf_all_occurrences_unprojected.empty:\n",
    "    # Get bounds from the geometry column of the GeoDataFrame\n",
    "    min_lon_all_data, min_lat_all_data, max_lon_all_data, max_lat_all_data = gdf_all_occurrences_unprojected.total_bounds\n",
    "    \n",
    "    lon_span = max_lon_all_data - min_lon_all_data\n",
    "    lat_span = max_lat_all_data - min_lat_all_data\n",
    "\n",
    "    # Add a 10% buffer, with a minimum of 1 degree\n",
    "    lon_buffer_overall = max(lon_span * 0.10, 1.0) \n",
    "    lat_buffer_overall = max(lat_span * 0.10, 1.0) \n",
    "    \n",
    "    overall_map_lim_min_lon = min_lon_all_data - lon_buffer_overall\n",
    "    overall_map_lim_max_lon = max_lon_all_data + lon_buffer_overall\n",
    "    overall_map_lim_min_lat = min_lat_all_data - lat_buffer_overall\n",
    "    overall_map_lim_max_lat = max_lat_all_data + lat_buffer_overall\n",
    "    print(f\"Calculated Overall Map Extent (EPSG:4326): Lon [{overall_map_lim_min_lon:.2f}, {overall_map_lim_max_lon:.2f}], Lat [{overall_map_lim_min_lat:.2f}, {overall_map_lim_max_lat:.2f}]\")\n",
    "else:\n",
    "    print(f\"Warning: Occurrence data is empty. Using default map extent: Lon [{overall_map_lim_min_lon:.2f}, {overall_map_lim_max_lon:.2f}], Lat [{overall_map_lim_min_lat:.2f}, {overall_map_lim_max_lat:.2f}]\")\n",
    "\n",
    "# Define Ticks based on the new overall extent (optional, but good for consistency)\n",
    "lon_tick_step = 10.0 # Adjust step as needed\n",
    "min_lon_tick = np.ceil(overall_map_lim_min_lon / lon_tick_step) * lon_tick_step\n",
    "max_lon_tick = np.floor(overall_map_lim_max_lon / lon_tick_step) * lon_tick_step\n",
    "lon_ticks = np.arange(min_lon_tick, max_lon_tick + lon_tick_step * 0.1, lon_tick_step)\n",
    "\n",
    "lat_tick_step = 5.0 # Adjust step as needed\n",
    "min_lat_tick = np.ceil(overall_map_lim_min_lat / lat_tick_step) * lat_tick_step\n",
    "max_lat_tick = np.floor(overall_map_lim_max_lat / lat_tick_step) * lat_tick_step\n",
    "lat_ticks = np.arange(min_lat_tick, max_lat_tick + lat_tick_step * 0.1, lat_tick_step)\n",
    "# --- End of Extent and Tick Calculation ---\n",
    "\n",
    "\n",
    "# --- Plotting Individual \"Flat\" Maps with Common Extent ---\n",
    "if gdf_all_occurrences_unprojected.empty or north_america_map_unprojected.empty or not species_colors or not unique_species_in_data:\n",
    "    print(\"Skipping flat maps due to missing data, basemap, colors, or species list after setup.\")\n",
    "else:\n",
    "    print(\"\\nGenerating individual 'flat' maps with common extent for each species...\")\n",
    "    new_marker_size = 30\n",
    "\n",
    "    for species_name_to_plot in unique_species_in_data:\n",
    "        print(f\"  Plotting map for: {species_name_to_plot}\")\n",
    "        \n",
    "        gdf_species_subset = gdf_all_occurrences_unprojected[\n",
    "            gdf_all_occurrences_unprojected['simpSciName'] == species_name_to_plot\n",
    "        ].copy() # Use .copy()\n",
    "\n",
    "        # Get the count of observations for the current species\n",
    "        observation_count = len(gdf_species_subset)\n",
    "        legend_label = f\"{species_name_to_plot} (n={observation_count})\"\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 9)) \n",
    "        \n",
    "        north_america_map_unprojected.plot(ax=ax, color='lightgray', edgecolor='black', linewidth=0.5, zorder=1)\n",
    "        \n",
    "        if not gdf_species_subset.empty:\n",
    "            gdf_species_subset.plot(\n",
    "                ax=ax, marker='o', \n",
    "                color=species_colors.get(species_name_to_plot, 'purple'), \n",
    "                markersize=new_marker_size, \n",
    "                alpha=0.7, \n",
    "                label=legend_label, # Use the new label with count\n",
    "                zorder=2\n",
    "            )\n",
    "        \n",
    "        ax.set_title(f'Distribution of {species_name_to_plot}', fontsize=14)\n",
    "        ax.set_xlabel(\"Longitude (degrees)\", fontsize=10)\n",
    "        ax.set_ylabel(\"Latitude (degrees)\", fontsize=10)\n",
    "        \n",
    "        ax.set_xlim(overall_map_lim_min_lon, overall_map_lim_max_lon)\n",
    "        ax.set_ylim(overall_map_lim_min_lat, overall_map_lim_max_lat)\n",
    "        ax.set_xticks(lon_ticks)\n",
    "        ax.set_yticks(lat_ticks)\n",
    "        \n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        if not gdf_species_subset.empty : \n",
    "            ax.legend(loc='best', markerscale=0.8)\n",
    "\n",
    "        plt.show()\n",
    "print(\"\\nIndividual flat species mapping process complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Plot all *Amorpha* spp. GBIF occurrence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 10: Amoprha spp. distirbution map ---\n",
    "\n",
    "# Check if essential variables are present\n",
    "missing_vars_final_map = []\n",
    "if 'gdf_all_occurrences_unprojected' not in globals() or gdf_all_occurrences_unprojected.empty:\n",
    "    missing_vars_final_map.append('gdf_all_occurrences_unprojected')\n",
    "if 'north_america_map_unprojected' not in globals() or north_america_map_unprojected.empty:\n",
    "    missing_vars_final_map.append('north_america_map_unprojected')\n",
    "if 'unique_species_in_data' not in globals() or not unique_species_in_data:\n",
    "    missing_vars_final_map.append('unique_species_in_data')\n",
    "if 'species_colors' not in globals() or not species_colors:\n",
    "    missing_vars_final_map.append('species_colors')\n",
    "if 'overall_map_lim_min_lon' not in globals(): # Check one of the extent vars\n",
    "    missing_vars_final_map.append('overall map limit variables (e.g., overall_map_lim_min_lon)')\n",
    "if 'new_marker_size' not in globals():\n",
    "    print(\"Warning: 'new_marker_size' not defined, defaulting to 48 for the final map.\")\n",
    "    new_marker_size = 48\n",
    "\n",
    "\n",
    "if missing_vars_final_map:\n",
    "    print(f\"Skipping final combined map due to missing prerequisite variables: {', '.join(missing_vars_final_map)}\")\n",
    "else:\n",
    "    print(\"\\nGenerating final 'flat' map with all species and a color key...\")\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 10)) # Adjusted figsize for map with legend\n",
    "\n",
    "    # 1. Plot the unprojected basemap\n",
    "    north_america_map_unprojected.plot(ax=ax, color='lightgray', edgecolor='black', linewidth=0.5, zorder=1)\n",
    "\n",
    "    # 2. Plot each species\n",
    "    for species_name_to_plot in unique_species_in_data:\n",
    "        gdf_species_subset = gdf_all_occurrences_unprojected[\n",
    "            gdf_all_occurrences_unprojected['simpSciName'] == species_name_to_plot\n",
    "        ]\n",
    "        \n",
    "        if not gdf_species_subset.empty:\n",
    "            gdf_species_subset.plot(\n",
    "                ax=ax, \n",
    "                marker='o', \n",
    "                color=species_colors.get(species_name_to_plot, 'grey'), # Use grey as fallback\n",
    "                markersize=new_marker_size, \n",
    "                alpha=0.7, \n",
    "                label=species_name_to_plot, # This label is used by ax.legend()\n",
    "                zorder=2\n",
    "            )\n",
    "\n",
    "    # 3. Set map title, labels, extent, and ticks\n",
    "    ax.set_title('Combined Distribution of All Amorpha Species', fontsize=16)\n",
    "    ax.set_xlabel(\"Longitude (degrees)\", fontsize=10)\n",
    "    ax.set_ylabel(\"Latitude (degrees)\", fontsize=10)\n",
    "    \n",
    "    ax.set_xlim(overall_map_lim_min_lon, overall_map_lim_max_lon)\n",
    "    ax.set_ylim(overall_map_lim_min_lat, overall_map_lim_max_lat)\n",
    "    \n",
    "    # Ensure lon_ticks and lat_ticks are defined from the previous step's extent calculation\n",
    "    if 'lon_ticks' in globals() and 'lat_ticks' in globals():\n",
    "        ax.set_xticks(lon_ticks)\n",
    "        ax.set_yticks(lat_ticks)\n",
    "    else:\n",
    "        print(\"Warning: lon_ticks or lat_ticks not defined. Using default ticks for the final map.\")\n",
    "        \n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # 4. Add Legend (Color Key)\n",
    "    # Adjust legend properties for better placement and appearance\n",
    "    legend = ax.legend(\n",
    "        title=\"Species\",\n",
    "        loc='best', # 'best' tries to find a good spot, or specify e.g. 'upper right'\n",
    "        markerscale=0.8, \n",
    "        fontsize='small',\n",
    "        title_fontsize='medium',\n",
    "        ncol=1 # Adjust number of columns if many species\n",
    "    )\n",
    "    if legend: # Ensure legend was created\n",
    "        legend.get_frame().set_alpha(0.9) # Make legend background slightly transparent\n",
    "\n",
    "    # Optional: Set aspect ratio for the flat map\n",
    "    # ax.set_aspect('equal', adjustable='box') # Uncomment if you want degrees to be visually ~square\n",
    "\n",
    "    plt.show()\n",
    "    print(\"\\nFinal combined species map generation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Observation thinning to reduce spatial autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 11: Spatial thinning to reduce spatial autocorrelation ---\n",
    "print(\"\\nPerforming spatial thinning for each species (0.25-degree grid)...\")\n",
    "gdf_thinned_results_list = []\n",
    "grid_resolution = 0.25 # degrees\n",
    "\n",
    "if 'gdf_all_occurrences_unprojected' in globals() and not gdf_all_occurrences_unprojected.empty:\n",
    "    for species_name_to_thin in unique_species_in_data:\n",
    "        species_gdf_original = gdf_all_occurrences_unprojected[\n",
    "            gdf_all_occurrences_unprojected['simpSciName'] == species_name_to_thin\n",
    "        ].copy()\n",
    "\n",
    "        if species_gdf_original.empty:\n",
    "            # If a species had no original data, add it as is (it will be empty)\n",
    "            # or ensure it has the 'thinning_status' column if needed for schema consistency later\n",
    "            species_gdf_original['thinning_status'] = pd.NA # Or 'kept' if it must be non-null\n",
    "            gdf_thinned_results_list.append(species_gdf_original)\n",
    "            continue\n",
    "\n",
    "        # Calculate grid cell IDs based on geometry\n",
    "        species_gdf_original['grid_lon_id'] = np.floor(species_gdf_original.geometry.x / grid_resolution)\n",
    "        species_gdf_original['grid_lat_id'] = np.floor(species_gdf_original.geometry.y / grid_resolution)\n",
    "        species_gdf_original['grid_cell_unique_id'] = species_gdf_original['grid_lon_id'].astype(str) + '_' + species_gdf_original['grid_lat_id'].astype(str)\n",
    "\n",
    "        # Randomly sample one point per grid cell for this species\n",
    "        # Using .index to ensure we are selecting from the original DataFrame's index\n",
    "        try:\n",
    "            kept_indices_for_species = species_gdf_original.groupby('grid_cell_unique_id', group_keys=False).apply(lambda x: x.sample(1, random_state=42)).index\n",
    "        except ValueError: # Happens if a group is empty, though sample(1) on a group of 1 should be fine\n",
    "            kept_indices_for_species = pd.Index([]) # No points kept if sampling fails (e.g. all groups were problematic)\n",
    "\n",
    "        species_gdf_original['thinning_status'] = 'dropped' # Default to dropped\n",
    "        species_gdf_original.loc[kept_indices_for_species, 'thinning_status'] = 'kept'\n",
    "        \n",
    "        gdf_thinned_results_list.append(species_gdf_original)\n",
    "\n",
    "    if gdf_thinned_results_list:\n",
    "        gdf_all_occurrences_thinned_status = pd.concat(gdf_thinned_results_list).reset_index(drop=True)\n",
    "        # Ensure it's still a GeoDataFrame if concatenation changed its type (unlikely with GeoPandas concat)\n",
    "        if not isinstance(gdf_all_occurrences_thinned_status, gpd.GeoDataFrame):\n",
    "             gdf_all_occurrences_thinned_status = gpd.GeoDataFrame(gdf_all_occurrences_thinned_status, geometry='geometry', crs=\"EPSG:4326\")\n",
    "        print(\"Spatial thinning complete.\")\n",
    "    else:\n",
    "        print(\"No data to thin or no species found. Creating an empty thinned DataFrame.\")\n",
    "        # Create an empty GDF with the expected columns if gdf_all_occurrences_unprojected was empty or no species\n",
    "        columns_if_empty = list(gdf_all_occurrences_unprojected.columns) + ['thinning_status'] if 'gdf_all_occurrences_unprojected' in globals() else ['geometry', 'simpSciName', 'thinning_status']\n",
    "        gdf_all_occurrences_thinned_status = gpd.GeoDataFrame(columns=columns_if_empty, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping thinning: 'gdf_all_occurrences_unprojected' is not defined or is empty.\")\n",
    "    # Ensure gdf_all_occurrences_thinned_status exists for subsequent plotting code, even if empty\n",
    "    gdf_all_occurrences_thinned_status = gpd.GeoDataFrame(columns=['geometry', 'simpSciName', 'thinning_status'], geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "# --- Plotting Individual \"Flat\" Maps with Thinned Data & SPECIES-SPECIFIC ZOOM ---\n",
    "if gdf_all_occurrences_thinned_status.empty or north_america_map_unprojected.empty or not species_colors or not unique_species_in_data:\n",
    "    print(\"Skipping thinned maps due to missing thinned data, basemap, colors, or species list.\")\n",
    "else:\n",
    "    print(\"\\nGenerating individual 'flat' maps with thinned data (zoomed to species extent) for each species...\")\n",
    "    kept_marker_size = 35\n",
    "    dropped_marker_size = 20\n",
    "    minimum_extent_buffer_degrees = 0.5 # Minimum buffer in degrees around data\n",
    "\n",
    "    for species_name_to_plot in unique_species_in_data:\n",
    "        print(f\"  Plotting thinned map for: {species_name_to_plot}\")\n",
    "        \n",
    "        species_subset_with_status = gdf_all_occurrences_thinned_status[\n",
    "            gdf_all_occurrences_thinned_status['simpSciName'] == species_name_to_plot\n",
    "        ].copy()\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 8)) # Adjust figsize as needed for potentially varying aspects\n",
    "        \n",
    "        # Plot the unprojected basemap\n",
    "        north_america_map_unprojected.plot(ax=ax, color='lightgray', edgecolor='black', linewidth=0.5, zorder=1)\n",
    "        \n",
    "        current_species_color = species_colors.get(species_name_to_plot, 'purple') # Default color\n",
    "\n",
    "        if species_subset_with_status.empty:\n",
    "            print(f\"    No data (kept or dropped) for {species_name_to_plot} to determine zoom. Using overall extent.\")\n",
    "            # Fallback to overall extent if no points for this species\n",
    "            ax.set_xlim(overall_map_lim_min_lon, overall_map_lim_max_lon)\n",
    "            ax.set_ylim(overall_map_lim_min_lat, overall_map_lim_max_lat)\n",
    "            ax.text(0.5, 0.5, 'No occurrence data for this species', \n",
    "                    horizontalalignment='center', verticalalignment='center', \n",
    "                    transform=ax.transAxes, fontsize=12, color='gray')\n",
    "        else:\n",
    "            gdf_kept = species_subset_with_status[species_subset_with_status['thinning_status'] == 'kept']\n",
    "            gdf_dropped = species_subset_with_status[species_subset_with_status['thinning_status'] == 'dropped']\n",
    "\n",
    "            count_kept = len(gdf_kept)\n",
    "            count_dropped = len(gdf_dropped)\n",
    "            label_kept = f\"Kept (n={count_kept})\"\n",
    "            label_dropped = f\"Dropped (n={count_dropped})\"\n",
    "\n",
    "            # Plot dropped points first\n",
    "            if not gdf_dropped.empty:\n",
    "                gdf_dropped.plot(\n",
    "                    ax=ax, marker='o', facecolors='none', edgecolors=current_species_color,\n",
    "                    linewidth=0.8, markersize=dropped_marker_size, alpha=0.4,\n",
    "                    label=label_dropped, zorder=2\n",
    "                )\n",
    "            \n",
    "            # Plot kept points on top\n",
    "            if not gdf_kept.empty:\n",
    "                gdf_kept.plot(\n",
    "                    ax=ax, marker='o', color=current_species_color, edgecolor='black',\n",
    "                    linewidth=0.3, markersize=kept_marker_size, alpha=0.8,\n",
    "                    label=label_kept, zorder=3\n",
    "                )\n",
    "\n",
    "            # Calculate species-specific extent for zooming\n",
    "            min_lon_species, min_lat_species, max_lon_species, max_lat_species = species_subset_with_status.total_bounds\n",
    "            \n",
    "            lon_span_species = max_lon_species - min_lon_species\n",
    "            lat_span_species = max_lat_species - min_lat_species\n",
    "            \n",
    "            # Dynamic buffer: 10% of span, or the minimum_extent_buffer_degrees, whichever is larger\n",
    "            lon_buffer_species = max(lon_span_species * 0.10, minimum_extent_buffer_degrees)\n",
    "            lat_buffer_species = max(lat_span_species * 0.10, minimum_extent_buffer_degrees)\n",
    "            \n",
    "            # Apply species-specific zoomed extent\n",
    "            ax.set_xlim(min_lon_species - lon_buffer_species, max_lon_species + lon_buffer_species)\n",
    "            ax.set_ylim(min_lat_species - lat_buffer_species, max_lat_species + lat_buffer_species)\n",
    "            \n",
    "            # Add legend\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            if handles:\n",
    "                ax.legend(handles, labels, loc='best', title=f\"{species_name_to_plot}\", markerscale=0.8, fontsize=9, title_fontsize=10)\n",
    "\n",
    "        ax.set_title(f'Spatially Thinned Distribution of {species_name_to_plot} (0.25° grid, Zoomed)', fontsize=14)\n",
    "        ax.set_xlabel(\"Longitude (degrees)\", fontsize=10)\n",
    "        ax.set_ylabel(\"Latitude (degrees)\", fontsize=10)\n",
    "        \n",
    "        ax.grid(True, linestyle='--', alpha=0.7) # Grid will adapt to auto-ticks\n",
    "\n",
    "        plt.show()\n",
    "print(\"\\nIndividual thinned species mapping process (zoomed) complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Import WorldClim 2.1 data (elevation and 19 BioClim)\n",
    "- https://www.worldclim.org/data/worldclim21.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 12: Import WorldClim raster data ---\n",
    "output_cropped_rasters = {} # To store cropped raster data and transforms\n",
    "\n",
    "# Define the bounding box from your species occurrence data\n",
    "# Ensure 'gdf_all_occurrences_thinned_status' is loaded and is a GeoDataFrame in EPSG:4326\n",
    "# For placeholder if the GDF isn't loaded yet:\n",
    "if 'gdf_all_occurrences_thinned_status' not in globals() or gdf_all_occurrences_thinned_status.empty:\n",
    "    print(\"Warning: 'gdf_all_occurrences_thinned_status' not found or empty.\")\n",
    "    print(\"Using a placeholder extent for North America for demonstration.\")\n",
    "    # Placeholder extent (roughly North America). Replace with your actual data bounds.\n",
    "    min_lon, min_lat, max_lon, max_lat = -170, 15, -50, 80 \n",
    "    # Create a placeholder GDF if needed for the code structure to run\n",
    "    placeholder_occurrence_data = pd.DataFrame({\n",
    "        'decimalLatitude': [min_lat, max_lat], \n",
    "        'decimalLongitude': [min_lon, max_lon],\n",
    "        'simpSciName': ['PlaceholderSpecies1', 'PlaceholderSpecies2'],\n",
    "        'thinning_status': ['kept', 'kept']\n",
    "    })\n",
    "    gdf_all_occurrences_thinned_status = gpd.GeoDataFrame(\n",
    "        placeholder_occurrence_data,\n",
    "        geometry=gpd.points_from_xy(placeholder_occurrence_data.decimalLongitude, \n",
    "                                     placeholder_occurrence_data.decimalLatitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "else:\n",
    "    if not isinstance(gdf_all_occurrences_thinned_status, gpd.GeoDataFrame):\n",
    "         print(\"Error: gdf_all_occurrences_thinned_status is not a GeoDataFrame.\")\n",
    "         # Handle error appropriately - perhaps load it or stop\n",
    "    elif gdf_all_occurrences_thinned_status.crs.to_string().upper() != 'EPSG:4326':\n",
    "        print(f\"Warning: gdf_all_occurrences_thinned_status CRS is {gdf_all_occurrences_thinned_status.crs}. WorldClim is EPSG:4326. Ensure they match or transform.\")\n",
    "        # It's crucial that occurrence data CRS matches raster CRS for extraction.\n",
    "        # WorldClim default is WGS84 (EPSG:4326).\n",
    "    \n",
    "    min_lon, min_lat, max_lon, max_lat = gdf_all_occurrences_thinned_status.total_bounds\n",
    "\n",
    "# Add a buffer to the bounding box (e.g., 0.5 degrees)\n",
    "buffer = 0.5 \n",
    "bbox_for_cropping = box(min_lon - buffer, min_lat - buffer, max_lon + buffer, max_lat + buffer)\n",
    "print(f\"Bounding box for cropping (with {buffer}° buffer): {bbox_for_cropping.bounds}\")\n",
    "\n",
    "# --- Function to load, crop, and return raster data ---\n",
    "def load_and_crop_raster(raster_path, geometry_to_crop, variable_name=\"raster\"):\n",
    "    \"\"\"\n",
    "    Loads a raster, crops it to the given geometry, and returns the data and transform.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(raster_path):\n",
    "        print(f\"Error: Raster file not found at {raster_path}\")\n",
    "        return None, None\n",
    "        \n",
    "    try:\n",
    "        with rasterio.open(raster_path) as src:\n",
    "            # Ensure the geometry is in the same CRS as the raster\n",
    "            # WorldClim is typically EPSG:4326 (WGS84). Assume geometry_to_crop is also.\n",
    "            if src.crs.to_string().upper() != 'EPSG:4326': # A basic check\n",
    "                 print(f\"Warning: Raster {variable_name} CRS ({src.crs}) may not be EPSG:4326. Cropping assumes matching CRSs.\")\n",
    "\n",
    "            cropped_data, cropped_transform = mask(src, [geometry_to_crop], crop=True)\n",
    "            \n",
    "            # Handle NoData: rasterio's mask sets nodata areas outside the geometry to 0 if crop=True.\n",
    "            # If the original raster has a specific NoData value, we might want to preserve it as NaN.\n",
    "            # For WorldClim, NoData is often a large negative number like -3.4e+38 or -9999.\n",
    "            # The `mask` function with `crop=True` should handle this by not including those areas,\n",
    "            # but within the valid crop, original NoData values need to be handled.\n",
    "            \n",
    "            nodata_val = src.nodata\n",
    "            if nodata_val is not None:\n",
    "                # Masked areas outside the shape are 0. Original nodata inside the shape needs to be NaN.\n",
    "                # Create a boolean mask for original NoData values\n",
    "                is_nodata = (cropped_data == nodata_val)\n",
    "                # Also treat the fill value from mask (0 for areas outside geometry if not originally 0) as NaN if appropriate\n",
    "                # This depends on whether 0 is a valid data value. For WorldClim temps, 0 C is valid.\n",
    "                # For precipitation, 0 mm is valid. So only original NoData should become NaN.\n",
    "                cropped_data = np.where(is_nodata, np.nan, cropped_data.astype(float)) # Convert to float for NaNs\n",
    "            else:\n",
    "                cropped_data = cropped_data.astype(float) # Ensure float for consistency\n",
    "\n",
    "            # Remove the first dimension if it's 1 (mask returns a 3D array)\n",
    "            if cropped_data.shape[0] == 1:\n",
    "                cropped_data = cropped_data.squeeze(axis=0)\n",
    "\n",
    "            print(f\"  Successfully loaded and cropped: {variable_name} ({os.path.basename(raster_path)})\")\n",
    "            print(f\"    Cropped shape: {cropped_data.shape}\")\n",
    "            print(f\"    Cropped min: {np.nanmin(cropped_data):.2f}, max: {np.nanmax(cropped_data):.2f} (after NaN conversion)\")\n",
    "            return cropped_data, cropped_transform\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {variable_name} ({raster_path}): {e}\")\n",
    "        return None, None\n",
    "\n",
    "# --- Process Bioclimatic Variables ---\n",
    "print(f\"\\nProcessing Bioclimatic variables from: {bio_vars_dir}\")\n",
    "if os.path.exists(bio_vars_dir):\n",
    "    for i in range(1, 20): # BIO1 to BIO19\n",
    "        var_name = f'bio{i}'\n",
    "        file_name = f'wc2.1_2.5m_bio_{i}.tif'\n",
    "        raster_path = os.path.join(bio_vars_dir, file_name)\n",
    "        \n",
    "        cropped_raster_data, cropped_raster_transform = load_and_crop_raster(raster_path, bbox_for_cropping, var_name)\n",
    "        \n",
    "        if cropped_raster_data is not None:\n",
    "            output_cropped_rasters[var_name] = {\n",
    "                'data': cropped_raster_data,\n",
    "                'transform': cropped_raster_transform\n",
    "            }\n",
    "else:\n",
    "    print(f\"Error: Bioclim directory not found at {bio_vars_dir}\")\n",
    "\n",
    "# --- Process Elevation ---\n",
    "print(f\"\\nProcessing Elevation data from: {elev_file_path}\")\n",
    "if os.path.exists(elev_file_path):\n",
    "    var_name = 'elev'\n",
    "    cropped_raster_data, cropped_raster_transform = load_and_crop_raster(elev_file_path, bbox_for_cropping, var_name)\n",
    "    \n",
    "    if cropped_raster_data is not None:\n",
    "        output_cropped_rasters[var_name] = {\n",
    "            'data': cropped_raster_data,\n",
    "            'transform': cropped_raster_transform\n",
    "        }\n",
    "else:\n",
    "    print(f\"Error: Elevation file not found at {elev_file_path}\")\n",
    "\n",
    "print(\"\\n--- Summary of Cropped Rasters ---\")\n",
    "if output_cropped_rasters:\n",
    "    for var_name, details in output_cropped_rasters.items():\n",
    "        print(f\"Variable: {var_name}, Shape: {details['data'].shape if details['data'] is not None else 'N/A'}\")\n",
    "else:\n",
    "    print(\"No rasters were processed or loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13a: Correlation matrix to reduce highly correlated (R^2 > 0.70) bioclim variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 13a: Correlation matrix to reduce highly correlated (R^2 > 0.70) bioclim variables ---\n",
    "\n",
    "# --- Bioclim Variable Names Mapping ---\n",
    "bioclim_names = {\n",
    "    'bio1': 'BIO1 Ann Mean Temp',\n",
    "    'bio2': 'BIO2 Mean Diurnal Range',\n",
    "    'bio3': 'BIO3 Isothermality',\n",
    "    'bio4': 'BIO4 Temp Seasonality',\n",
    "    'bio5': 'BIO5 Max Temp Warm Mth',\n",
    "    'bio6': 'BIO6 Min Temp Cold Mth',\n",
    "    'bio7': 'BIO7 Temp Ann Range',\n",
    "    'bio8': 'BIO8 Mean Temp Wet Qtr',\n",
    "    'bio9': 'BIO9 Mean Temp Dry Qtr',\n",
    "    'bio10': 'BIO10 Mean Temp Warm Qtr',\n",
    "    'bio11': 'BIO11 Mean Temp Cold Qtr',\n",
    "    'bio12': 'BIO12 Ann Precip',\n",
    "    'bio13': 'BIO13 Precip Wet Mth',\n",
    "    'bio14': 'BIO14 Precip Dry Mth',\n",
    "    'bio15': 'BIO15 Precip Seasonality',\n",
    "    'bio16': 'BIO16 Precip Wet Qtr',\n",
    "    'bio17': 'BIO17 Precip Dry Qtr',\n",
    "    'bio18': 'BIO18 Precip Warm Qtr',\n",
    "    'bio19': 'BIO19 Precip Cold Qtr',\n",
    "    'elev': 'Elevation'\n",
    "}\n",
    "\n",
    "# --- 1. Extract and Prepare Data for Analysis ---\n",
    "data_for_analysis = {}\n",
    "reference_shape = None\n",
    "reference_var_name = None\n",
    "\n",
    "print(\"Preparing bioclimatic and elevation data for analysis...\")\n",
    "\n",
    "if 'output_cropped_rasters' not in globals() or not output_cropped_rasters:\n",
    "    print(\"Error: 'output_cropped_rasters' is not defined or is empty.\")\n",
    "    output_cropped_rasters = {}\n",
    "\n",
    "analysis_keys = sorted([key for key in output_cropped_rasters.keys() if key.startswith('bio') or key == 'elev'])\n",
    "\n",
    "if not analysis_keys:\n",
    "    print(\"No bioclimatic or elevation variables found in 'output_cropped_rasters'.\")\n",
    "else:\n",
    "    for var_name_short in analysis_keys: \n",
    "        if var_name_short in output_cropped_rasters and \\\n",
    "           'data' in output_cropped_rasters[var_name_short] and \\\n",
    "           output_cropped_rasters[var_name_short]['data'] is not None:\n",
    "            \n",
    "            current_data = output_cropped_rasters[var_name_short]['data']\n",
    "            if reference_shape is None:\n",
    "                reference_shape = current_data.shape\n",
    "                reference_var_name = var_name_short\n",
    "                print(f\"  Using shape of '{bioclim_names.get(var_name_short, var_name_short)}' ({reference_shape}) as reference.\")\n",
    "            \n",
    "            if current_data.shape != reference_shape:\n",
    "                print(f\"  Warning: Shape of '{bioclim_names.get(var_name_short, var_name_short)}' ({current_data.shape}) \"\n",
    "                      f\"does not match reference shape ({reference_shape} from '{bioclim_names.get(reference_var_name, reference_var_name)}').\")\n",
    "                print(f\"  Skipping '{bioclim_names.get(var_name_short, var_name_short)}' for analysis.\")\n",
    "                continue\n",
    "            \n",
    "            descriptive_name = bioclim_names.get(var_name_short, var_name_short)\n",
    "            data_for_analysis[descriptive_name] = current_data.flatten()\n",
    "        else:\n",
    "            print(f\"  Data for '{bioclim_names.get(var_name_short, var_name_short)}' is missing or None. Skipping.\")\n",
    "\n",
    "# --- 2. Create a DataFrame with Descriptive Names ---\n",
    "if not data_for_analysis:\n",
    "    print(\"No valid data to build DataFrame for analysis. Aborting.\")\n",
    "else:\n",
    "    df_analysis = pd.DataFrame(data_for_analysis)\n",
    "    ordered_descriptive_names = [bioclim_names.get(k, k) for k in analysis_keys if bioclim_names.get(k,k) in df_analysis.columns]\n",
    "    df_analysis = df_analysis[ordered_descriptive_names]\n",
    "\n",
    "    print(f\"\\nDataFrame created with {len(df_analysis.columns)} variables and {len(df_analysis)} pixels (rows).\")\n",
    "\n",
    "    # --- 3. Handle NoData/NaN values ---\n",
    "    original_pixel_count = len(df_analysis)\n",
    "    df_analysis_cleaned = df_analysis.dropna()\n",
    "    cleaned_pixel_count = len(df_analysis_cleaned)\n",
    "    print(f\"  Removed {original_pixel_count - cleaned_pixel_count} pixels with NaN values.\")\n",
    "    print(f\"  {cleaned_pixel_count} pixels remain for analysis.\")\n",
    "\n",
    "    if cleaned_pixel_count < 2:\n",
    "        print(\"  Error: Not enough data points after NaN removal for analysis.\")\n",
    "    elif len(df_analysis_cleaned.columns) < 2:\n",
    "        print(f\"  Error: Only {len(df_analysis_cleaned.columns)} variable(s) remain. Need at least 2 for pair plots.\")\n",
    "    else:\n",
    "        # --- 4. Correlation Matrix (Heatmap) ---\n",
    "        # (Heatmap code remains the same as previous correct version)\n",
    "        print(\"\\nCalculating correlation matrix...\")\n",
    "        correlation_matrix_final = df_analysis_cleaned.corr(method='pearson')\n",
    "\n",
    "        print(\"\\n--- Correlation Matrix (Descriptive Names) ---\")\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 200):\n",
    "            print(correlation_matrix_final)\n",
    "\n",
    "        print(\"\\nVisualizing correlation matrix (heatmap)...\")\n",
    "        plt.figure(figsize=(20, 16))\n",
    "        sns.heatmap(correlation_matrix_final, annot=True, cmap='coolwarm', fmt=\".2f\", \n",
    "                    linewidths=.5, vmin=-1, vmax=1,\n",
    "                    cbar_kws={'label': 'Pearson Correlation Coefficient'})\n",
    "        plt.title('Correlation Matrix of Cropped Climate Variables', fontsize=18)\n",
    "        plt.xticks(rotation=60, ha='right', fontsize=9)\n",
    "        plt.yticks(rotation=0, fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # --- 5. Scatter Plot Matrix with Regression Lines and R-squared ---\n",
    "        print(\"\\nGenerating scatter plot matrix (this might take time)...\")\n",
    "        \n",
    "        sample_size_for_pairplot = min(1000, len(df_analysis_cleaned))\n",
    "        df_for_pairplot = df_analysis_cleaned.sample(n=sample_size_for_pairplot, random_state=42) if len(df_analysis_cleaned) > sample_size_for_pairplot else df_analysis_cleaned\n",
    "        print(f\"  Using {len(df_for_pairplot)} sampled pixels for the pairplot.\")\n",
    "\n",
    "        def annotate_r_squared(x, y, ax=None, **kwargs):\n",
    "            if not isinstance(x, pd.Series) or not isinstance(y, pd.Series):\n",
    "                return\n",
    "            valid_indices = ~x.isnull() & ~y.isnull()\n",
    "            x_valid = x[valid_indices]\n",
    "            y_valid = y[valid_indices]\n",
    "            if len(x_valid) < 2 or len(y_valid) < 2:\n",
    "                r_squared_text = \"R²: N/A\"\n",
    "            else:\n",
    "                r, _ = pearsonr(x_valid, y_valid)\n",
    "                r_squared = r**2\n",
    "                r_squared_text = f'$R^2={r_squared:.2f}$'\n",
    "            ax = ax or plt.gca()\n",
    "            ax.text(0.05, 0.9, r_squared_text, transform=ax.transAxes, \n",
    "                    fontsize=8, ha='left', va='top', \n",
    "                    bbox=dict(boxstyle='round,pad=0.3', fc='wheat', alpha=0.5))\n",
    "\n",
    "        # MODIFIED HERE: Removed plot_kws from the main pairplot call\n",
    "        pair_plot = sns.pairplot(df_for_pairplot, \n",
    "                                 diag_kind='kde',\n",
    "                                 diag_kws={'fill': True, 'alpha':0.6}) \n",
    "        \n",
    "        # Apply regression plot to lower triangle and R-squared to upper\n",
    "        pair_plot.map_lower(sns.regplot, \n",
    "                            scatter_kws={'alpha':0.3, 's':10},  # Style for regplot's scatter\n",
    "                            line_kws={'color':'red', 'linewidth':1}) # Style for regplot's line\n",
    "        pair_plot.map_upper(annotate_r_squared)\n",
    "        \n",
    "        for ax_row in pair_plot.axes:\n",
    "            for ax_col in ax_row:\n",
    "                ax_col.tick_params(axis='both', which='major', labelsize=7)\n",
    "                ax_col.xaxis.label.set_size(8) \n",
    "                ax_col.yaxis.label.set_size(8)\n",
    "\n",
    "        pair_plot.fig.suptitle('Scatter Plot Matrix of Climate Variables (Regression & R²)', y=1.02, fontsize=18)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nAnalysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13b: Plot un-correlated bioclim variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 13b: Plot un-correlated bioclim variables ---\n",
    "\n",
    "# --- Bioclim Variable Names Mapping (ensure it's available) ---\n",
    "bioclim_names = {\n",
    "    'bio1': 'BIO1 Ann Mean Temp', 'bio2': 'BIO2 Mean Diurnal Range',\n",
    "    'bio3': 'BIO3 Isothermality', 'bio4': 'BIO4 Temp Seasonality',\n",
    "    'bio5': 'BIO5 Max Temp Warm Mth', 'bio6': 'BIO6 Min Temp Cold Mth',\n",
    "    'bio7': 'BIO7 Temp Ann Range', 'bio8': 'BIO8 Mean Temp Wet Qtr',\n",
    "    'bio9': 'BIO9 Mean Temp Dry Qtr', 'bio10': 'BIO10 Mean Temp Warm Qtr',\n",
    "    'bio11': 'BIO11 Mean Temp Cold Qtr', 'bio12': 'BIO12 Ann Precip',\n",
    "    'bio13': 'BIO13 Precip Wet Mth', 'bio14': 'BIO14 Precip Dry Mth',\n",
    "    'bio15': 'BIO15 Precip Seasonality', 'bio16': 'BIO16 Precip Wet Qtr',\n",
    "    'bio17': 'BIO17 Precip Dry Qtr', 'bio18': 'BIO18 Precip Warm Qtr',\n",
    "    'bio19': 'BIO19 Precip Cold Qtr', 'elev': 'Elevation'\n",
    "}\n",
    "\n",
    "# Ensure df_analysis_cleaned is defined from the previous step\n",
    "if 'df_analysis_cleaned' not in globals() or df_analysis_cleaned.empty:\n",
    "    print(\"Error: 'df_analysis_cleaned' is not defined or is empty. Please run the previous cell.\")\n",
    "    # As a fallback for standalone execution, let's try to recreate it if possible\n",
    "    # This assumes 'output_cropped_rasters' is available\n",
    "    if 'output_cropped_rasters' in globals() and output_cropped_rasters:\n",
    "        data_for_analysis = {}\n",
    "        analysis_keys = sorted([key for key in output_cropped_rasters.keys() if key.startswith('bio') or key == 'elev'])\n",
    "        reference_shape = None\n",
    "        if analysis_keys:\n",
    "            for var_name_short in analysis_keys:\n",
    "                if var_name_short in output_cropped_rasters and output_cropped_rasters[var_name_short]['data'] is not None:\n",
    "                    current_data = output_cropped_rasters[var_name_short]['data']\n",
    "                    if reference_shape is None: reference_shape = current_data.shape\n",
    "                    if current_data.shape == reference_shape:\n",
    "                         data_for_analysis[bioclim_names.get(var_name_short, var_name_short)] = current_data.flatten()\n",
    "            df_analysis = pd.DataFrame(data_for_analysis)\n",
    "            ordered_descriptive_names = [bioclim_names.get(k, k) for k in analysis_keys if bioclim_names.get(k,k) in df_analysis.columns]\n",
    "            df_analysis = df_analysis[ordered_descriptive_names]\n",
    "            df_analysis_cleaned = df_analysis.dropna()\n",
    "            if df_analysis_cleaned.empty: print(\"Recreated df_analysis_cleaned is empty.\")\n",
    "        else: print(\"No keys found to recreate df_analysis_cleaned.\")\n",
    "    else:\n",
    "        print(\"Cannot recreate df_analysis_cleaned as output_cropped_rasters is also missing.\")\n",
    "        # You might want to raise an error or exit here if it cannot be recreated.\n",
    "        # For now, this script might fail if it's not available.\n",
    "\n",
    "\n",
    "if 'df_analysis_cleaned' in globals() and not df_analysis_cleaned.empty and len(df_analysis_cleaned.columns) >=2 :\n",
    "    print(f\"Starting with {len(df_analysis_cleaned.columns)} variables.\")\n",
    "\n",
    "    # --- 1. Calculate the Full Correlation Matrix ---\n",
    "    correlation_matrix_full = df_analysis_cleaned.corr(method='pearson')\n",
    "    \n",
    "    # --- 2. Iteratively Identify and Mark Variables for Removal ---\n",
    "    variables_to_remove = set()\n",
    "    threshold = 0.70\n",
    "    \n",
    "    # Create a copy of column names to iterate over, as we might modify the set of considered columns indirectly\n",
    "    columns_to_consider = list(df_analysis_cleaned.columns) \n",
    "\n",
    "    # Calculate sum of absolute correlations for each variable with all others (in the original full set)\n",
    "    # This helps decide which of a correlated pair is \"more connected\" overall\n",
    "    sum_abs_corr = correlation_matrix_full.abs().sum(axis=1) - 1 # Subtract 1 for self-correlation\n",
    "\n",
    "    # Iterate through the upper triangle of the correlation matrix\n",
    "    for i in range(len(correlation_matrix_full.columns)):\n",
    "        for j in range(i + 1, len(correlation_matrix_full.columns)):\n",
    "            var_i = correlation_matrix_full.columns[i]\n",
    "            var_j = correlation_matrix_full.columns[j]\n",
    "            \n",
    "            # If one of the variables is already marked for removal, skip this pair\n",
    "            if var_i in variables_to_remove or var_j in variables_to_remove:\n",
    "                continue\n",
    "                \n",
    "            correlation_value = correlation_matrix_full.loc[var_i, var_j]\n",
    "            \n",
    "            if abs(correlation_value) > threshold:\n",
    "                print(f\"  Found correlation > {threshold}: '{var_i}' and '{var_j}' ({correlation_value:.2f})\")\n",
    "                \n",
    "                # Decide which variable to remove: the one with higher sum_abs_corr\n",
    "                if sum_abs_corr[var_i] > sum_abs_corr[var_j]:\n",
    "                    print(f\"    Marking '{var_i}' for removal (higher overall abs corr: {sum_abs_corr[var_i]:.2f} vs {sum_abs_corr[var_j]:.2f} for '{var_j}').\")\n",
    "                    variables_to_remove.add(var_i)\n",
    "                elif sum_abs_corr[var_j] > sum_abs_corr[var_i]:\n",
    "                    print(f\"    Marking '{var_j}' for removal (higher overall abs corr: {sum_abs_corr[var_j]:.2f} vs {sum_abs_corr[var_i]:.2f} for '{var_i}').\")\n",
    "                    variables_to_remove.add(var_j)\n",
    "                else:\n",
    "                    # Tie-breaking: remove the one that comes later alphabetically or by original order\n",
    "                    # For simplicity, let's remove the one later in the current column list\n",
    "                    # This is arbitrary but consistent.\n",
    "                    if columns_to_consider.index(var_i) > columns_to_consider.index(var_j):\n",
    "                         print(f\"    Tie in overall abs corr. Marking '{var_i}' for removal (appears later).\")\n",
    "                         variables_to_remove.add(var_i)\n",
    "                    else:\n",
    "                         print(f\"    Tie in overall abs corr. Marking '{var_j}' for removal (appears later).\")\n",
    "                         variables_to_remove.add(var_j)\n",
    "\n",
    "\n",
    "    print(f\"\\nVariables marked for removal: {variables_to_remove if variables_to_remove else 'None'}\")\n",
    "\n",
    "    # --- 3. Create DataFrame with Reduced Set of Variables ---\n",
    "    variables_to_keep = [col for col in df_analysis_cleaned.columns if col not in variables_to_remove]\n",
    "    df_analysis_reduced = df_analysis_cleaned[variables_to_keep]\n",
    "    \n",
    "    print(f\"\\nReduced set has {len(df_analysis_reduced.columns)} variables: {df_analysis_reduced.columns.tolist()}\")\n",
    "\n",
    "    if len(df_analysis_reduced.columns) < 2:\n",
    "        print(\"  Error: After removing correlated variables, less than 2 variables remain. Cannot create correlation matrix.\")\n",
    "    else:\n",
    "        # --- 4. Calculate and Display Correlation Matrix for Reduced Set ---\n",
    "        print(\"\\nCalculating correlation matrix for the reduced set of variables...\")\n",
    "        correlation_matrix_reduced = df_analysis_reduced.corr(method='pearson')\n",
    "\n",
    "        print(\"\\n--- Correlation Matrix (Reduced Set) ---\")\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 200):\n",
    "            print(correlation_matrix_reduced)\n",
    "\n",
    "        print(\"\\nVisualizing correlation matrix (heatmap) for the reduced set...\")\n",
    "        plt.figure(figsize=(max(10, len(df_analysis_reduced.columns)*0.8), max(8, len(df_analysis_reduced.columns)*0.6)))\n",
    "        sns.heatmap(correlation_matrix_reduced, annot=True, cmap='coolwarm', fmt=\".2f\", \n",
    "                    linewidths=.5, vmin=-1, vmax=1,\n",
    "                    cbar_kws={'label': 'Pearson Correlation Coefficient'})\n",
    "        plt.title('Correlation Matrix of Reduced Climate Variables (Threshold > 0.70)', fontsize=16)\n",
    "        plt.xticks(rotation=60, ha='right', fontsize=9)\n",
    "        plt.yticks(rotation=0, fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # --- 5. Scatter Plot Matrix for Reduced Set (Optional, can be slow) ---\n",
    "        # Consider if you still need this for the reduced set.\n",
    "        # If df_analysis_reduced.columns is small (e.g., < 8), it might be quick.\n",
    "        if len(df_analysis_reduced.columns) > 1 and len(df_analysis_reduced.columns) <= 10: # Limit for practical pairplot\n",
    "            print(\"\\nGenerating scatter plot matrix for the reduced set (this might take some time)...\")\n",
    "            \n",
    "            sample_size_for_pairplot_reduced = min(1000, len(df_analysis_reduced))\n",
    "            df_for_pairplot_reduced = df_analysis_reduced.sample(n=sample_size_for_pairplot_reduced, random_state=42) if len(df_analysis_reduced) > sample_size_for_pairplot_reduced else df_analysis_reduced\n",
    "            print(f\"  Using {len(df_for_pairplot_reduced)} sampled pixels for the reduced pairplot.\")\n",
    "\n",
    "            def annotate_r_squared_reduced(x, y, ax=None, **kwargs): # Copied from previous\n",
    "                if not isinstance(x, pd.Series) or not isinstance(y, pd.Series): return\n",
    "                valid_indices = ~x.isnull() & ~y.isnull()\n",
    "                x_valid, y_valid = x[valid_indices], y[valid_indices]\n",
    "                if len(x_valid) < 2 or len(y_valid) < 2: r_squared_text = \"R²: N/A\"\n",
    "                else: r, _ = pearsonr(x_valid, y_valid); r_squared = r**2; r_squared_text = f'$R^2={r_squared:.2f}$'\n",
    "                ax = ax or plt.gca()\n",
    "                ax.text(0.05, 0.9, r_squared_text, transform=ax.transAxes, fontsize=8, ha='left', va='top', bbox=dict(boxstyle='round,pad=0.3', fc='wheat', alpha=0.5))\n",
    "\n",
    "            pair_plot_reduced = sns.pairplot(df_for_pairplot_reduced, diag_kind='kde', diag_kws={'fill': True, 'alpha':0.6})\n",
    "            pair_plot_reduced.map_lower(sns.regplot, scatter_kws={'alpha':0.3, 's':10}, line_kws={'color':'red', 'linewidth':1})\n",
    "            pair_plot_reduced.map_upper(annotate_r_squared_reduced)\n",
    "\n",
    "            for ax_r in pair_plot_reduced.axes:\n",
    "                for ax_c in ax_r:\n",
    "                    ax_c.tick_params(axis='both', which='major', labelsize=7)\n",
    "                    ax_c.xaxis.label.set_size(8); ax_c.yaxis.label.set_size(8)\n",
    "            \n",
    "            pair_plot_reduced.fig.suptitle('Scatter Plot Matrix of Reduced Climate Variables (Regression & R²)', y=1.02, fontsize=16)\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "            plt.show()\n",
    "        elif len(df_analysis_reduced.columns) > 10:\n",
    "            print(\"\\nSkipping pairplot for the reduced set as it still has many variables (>10). Heatmap should suffice.\")\n",
    "        \n",
    "    print(\"\\nVariable reduction process complete.\")\n",
    "else:\n",
    "    print(\"Skipping variable reduction as 'df_analysis_cleaned' is not suitable (e.g. empty or too few columns).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13c: Plot reduced BioClim variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 13c: Plot reduced BioClim variables ---\n",
    "# Invert the bioclim_names for easy lookup from descriptive to short key\n",
    "short_names_map = {v: k for k, v in bioclim_names.items()}\n",
    "\n",
    "# --- Units for variables (as provided by user) ---\n",
    "variable_units = {\n",
    "    'BIO10 Mean Temp Warm Qtr': '°C', # bio10\n",
    "    'BIO13 Precip Wet Mth': 'mm',     # bio13\n",
    "    'BIO15 Precip Seasonality': 'CV', # bio15 (Coefficient of Variation)\n",
    "    'BIO18 Precip Warm Qtr': 'mm',    # bio18\n",
    "    'BIO2 Mean Diurnal Range': '°C',  # bio2\n",
    "    'BIO8 Mean Temp Wet Qtr': '°C',   # bio8\n",
    "    'BIO9 Mean Temp Dry Qtr': '°C',   # bio9\n",
    "    'Elevation': 'meters'             # elev\n",
    "}\n",
    "# Add any other variables that might be in df_analysis_reduced with their units\n",
    "# If a variable from df_analysis_reduced is not in variable_units, it will plot without unit in cbar label.\n",
    "\n",
    "\n",
    "# Check if df_analysis_reduced exists\n",
    "if 'df_analysis_reduced' not in globals() or df_analysis_reduced.empty:\n",
    "    print(\"Error: 'df_analysis_reduced' (DataFrame with selected variables) not found or is empty.\")\n",
    "    print(\"Please ensure the variable reduction step has been run successfully.\")\n",
    "    # Create a placeholder if needed for the code to run partially\n",
    "    class PlaceholderDF:\n",
    "        columns = []\n",
    "    df_analysis_reduced = PlaceholderDF()\n",
    "\n",
    "\n",
    "if 'output_cropped_rasters' not in globals() or not output_cropped_rasters:\n",
    "    print(\"Error: 'output_cropped_rasters' not found or is empty.\")\n",
    "    # Create a placeholder\n",
    "    output_cropped_rasters = {}\n",
    "\n",
    "if 'north_america_map_unprojected' not in globals():\n",
    "    print(\"Warning: 'north_america_map_unprojected' not found. Basemap will not be plotted.\")\n",
    "    north_america_map_unprojected = None # Avoid error later\n",
    "\n",
    "if 'gdf_all_occurrences_thinned_status' not in globals():\n",
    "    print(\"Warning: 'gdf_all_occurrences_thinned_status' not found. Occurrence points will not be plotted.\")\n",
    "    gdf_all_occurrences_thinned_status = None\n",
    "\n",
    "\n",
    "# --- Plotting Selected Variables ---\n",
    "final_selected_variables_desc = list(df_analysis_reduced.columns)\n",
    "\n",
    "if not final_selected_variables_desc:\n",
    "    print(\"No variables in the reduced set to plot.\")\n",
    "else:\n",
    "    print(f\"\\nPlotting {len(final_selected_variables_desc)} selected climate variables over their cropped extent...\")\n",
    "    \n",
    "    # Determine overall extent for consistent plotting limits across these variables\n",
    "    # This should be based on the extent used for cropping them originally.\n",
    "    # Re-derive from one of the cropped rasters if not stored, or use `bbox_for_cropping`\n",
    "    # from the raster processing step if it's still in scope.\n",
    "    \n",
    "    # For simplicity, let's try to get extent from the first available raster\n",
    "    # or fallback to the bbox_for_cropping if defined.\n",
    "    plot_extent = None \n",
    "    first_valid_short_key = None\n",
    "\n",
    "    for desc_name in final_selected_variables_desc:\n",
    "        short_key = short_names_map.get(desc_name)\n",
    "        \n",
    "        if not short_key or short_key not in output_cropped_rasters or \\\n",
    "           output_cropped_rasters[short_key]['data'] is None:\n",
    "            print(f\"  Skipping '{desc_name}': Data not found in output_cropped_rasters or is None.\")\n",
    "            continue\n",
    "\n",
    "        raster_data = output_cropped_rasters[short_key]['data']\n",
    "        # raster_transform is still needed for plotting_extent, but not for imshow's transform kwarg\n",
    "        raster_transform_for_extent = output_cropped_rasters[short_key]['transform'] \n",
    "        unit = variable_units.get(desc_name, '')\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "        \n",
    "        current_cmap_name = 'viridis' # Default\n",
    "        if short_key == 'elev':\n",
    "            current_cmap_name = 'terrain' # Specific colormap for elevation\n",
    "        elif 'precip' in desc_name.lower() or 'bio12' <= short_key <= 'bio19': # Precipitation related\n",
    "            current_cmap_name = 'Blues' # Or 'GnBu', 'YlGnBu'\n",
    "        elif 'temp' in desc_name.lower() or 'bio1' <= short_key <= 'bio11': # Temperature related\n",
    "            current_cmap_name = 'coolwarm' # Or 'RdYlBu_r' for diverging\n",
    "\n",
    "        cmap = plt.get_cmap(current_cmap_name) # Use plt.get_cmap to get the colormap object\n",
    "        cmap.set_bad(color='lightgray')\n",
    "\n",
    "        # MODIFIED HERE: Removed 'transform=raster_transform'\n",
    "        im = ax.imshow(raster_data, \n",
    "                       cmap=cmap, \n",
    "                       extent=rasterio.plot.plotting_extent(raster_data, raster_transform_for_extent) if raster_data is not None else None\n",
    "                      )\n",
    "\n",
    "        # Plot basemap\n",
    "        if north_america_map_unprojected is not None and plot_extent_xmin is not None:\n",
    "            try:\n",
    "                north_america_map_unprojected.plot(ax=ax, color='lightgoldenrodyellow', edgecolor='gray', linewidth=0.5, zorder=1, alpha=0.7)\n",
    "            except Exception as e:\n",
    "                print(f\"    Could not plot/clip basemap for {desc_name}: {e}\")\n",
    "        \n",
    "        # Overlay occurrence points\n",
    "        if gdf_all_occurrences_thinned_status is not None and not gdf_all_occurrences_thinned_status.empty:\n",
    "            gdf_all_occurrences_thinned_status.plot(ax=ax, marker='.', color='red', markersize=5, zorder=3, alpha=0.6, label='Occurrences')\n",
    "\n",
    "        # Set plot limits\n",
    "        if plot_extent_xmin is not None:\n",
    "            ax.set_xlim(plot_extent_xmin, plot_extent_xmax)\n",
    "            ax.set_ylim(plot_extent_ymin, plot_extent_ymax)\n",
    "\n",
    "        ax.set_title(f'{desc_name}', fontsize=14)\n",
    "        ax.set_xlabel(\"Longitude\", fontsize=10)\n",
    "        ax.set_ylabel(\"Latitude\", fontsize=10)\n",
    "        ax.grid(True, linestyle='--', alpha=0.5)\n",
    "        \n",
    "        cbar = fig.colorbar(im, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "        cbar.set_label(f'{unit}', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "print(\"\\nFinished plotting selected climate variables.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14a: Extract climate vars for all occurrence data and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 14: Extract climate vars for all occurrence data and visualize ---\n",
    "\n",
    "# --- Define species_colors (ensure this is done consistently, e.g., from an earlier cell) ---\n",
    "# This should be defined based on the species present in your main occurrence dataset\n",
    "# gdf_all_occurrences_thinned_status or a similar definitive list.\n",
    "if 'species_colors' not in globals():\n",
    "    print(\"Warning: 'species_colors' not defined. Attempting to create it.\")\n",
    "    if 'gdf_all_occurrences_thinned_status' in globals() and \\\n",
    "       'simpSciName' in gdf_all_occurrences_thinned_status.columns and \\\n",
    "       not gdf_all_occurrences_thinned_status.empty:\n",
    "        \n",
    "        unique_species_for_colors = sorted(list(gdf_all_occurrences_thinned_status['simpSciName'].dropna().unique()))\n",
    "        if unique_species_for_colors:\n",
    "            try:\n",
    "                palette_gen = sns.color_palette(\"husl\", len(unique_species_for_colors))\n",
    "            except Exception: # Fallback if seaborn fails or husl changes\n",
    "                cmap_gen = plt.cm.get_cmap('tab10', max(1,len(unique_species_for_colors))) # Ensure at least 1 color\n",
    "                palette_gen = [cmap_gen(i) for i in range(len(unique_species_for_colors))]\n",
    "            species_colors = {name: palette_gen[i] for i, name in enumerate(unique_species_for_colors)}\n",
    "            print(f\"Defined 'species_colors' for {len(species_colors)} species.\")\n",
    "        else:\n",
    "            print(\"Could not define 'species_colors': No unique species names found in thinned data.\")\n",
    "            species_colors = {}\n",
    "    else:\n",
    "        print(\"Could not define 'species_colors': 'gdf_all_occurrences_thinned_status' or 'simpSciName' column missing/empty.\")\n",
    "        species_colors = {}\n",
    "# --- End of species_colors definition ---\n",
    "\n",
    "\n",
    "# --- Placeholder checks for other prerequisites (if running standalone) ---\n",
    "if 'gdf_all_occurrences_thinned_status' not in globals() or gdf_all_occurrences_thinned_status.empty:\n",
    "    print(\"Error: gdf_all_occurrences_thinned_status is not defined or empty.\")\n",
    "    gdf_all_occurrences_thinned_status = gpd.GeoDataFrame(\n",
    "        {'simpSciName': ['Amorpha test'], 'geometry': [box(-100, 30, -99, 31)]}, crs=\"EPSG:4326\"\n",
    "    )\n",
    "if 'output_cropped_rasters' not in globals() or not output_cropped_rasters:\n",
    "    print(\"Error: output_cropped_rasters is not defined or empty.\")\n",
    "    output_cropped_rasters = {}\n",
    "if 'df_analysis_reduced' not in globals() or df_analysis_reduced.empty:\n",
    "    print(\"Error: df_analysis_reduced (selected climate variables) is not defined or empty.\")\n",
    "    class PlaceholderReducedDF: columns = []\n",
    "    df_analysis_reduced = PlaceholderReducedDF()\n",
    "if 'bioclim_names' not in globals(): \n",
    "    bioclim_names = {f'bio{i}': f'BIO{i}' for i in range(1,20)}; bioclim_names['elev'] = 'Elevation'\n",
    "if 'short_names_map' not in globals(): \n",
    "    short_names_map = {v: k for k, v in bioclim_names.items()}\n",
    "if 'variable_units' not in globals(): \n",
    "    variable_units = {name: 'units' for name in bioclim_names.values()}\n",
    "\n",
    "\n",
    "# --- Part 1: Extract Climate Data for Species Occurrence Points ---\n",
    "print(\"\\n--- Part 1: Extracting Climate Data for Occurrence Points ---\")\n",
    "gdf_occurrences_with_climate = gdf_all_occurrences_thinned_status.copy()\n",
    "selected_vars_descriptive = list(df_analysis_reduced.columns)\n",
    "\n",
    "if not selected_vars_descriptive:\n",
    "    print(\"No selected climate variables found in df_analysis_reduced. Skipping extraction and subsequent analyses.\")\n",
    "    if 'gdf_occurrences_with_climate' not in globals(): gdf_occurrences_with_climate = pd.DataFrame()\n",
    "else:\n",
    "    if 'geometry' not in gdf_occurrences_with_climate.columns or not isinstance(gdf_occurrences_with_climate.geometry, gpd.GeoSeries):\n",
    "        print(\"Error: 'geometry' column missing or invalid in gdf_all_occurrences_thinned_status.\")\n",
    "        selected_vars_descriptive = [] \n",
    "    elif gdf_occurrences_with_climate.crs is None or gdf_occurrences_with_climate.crs.to_string().upper() != 'EPSG:4326':\n",
    "        print(f\"Warning: Occurrence CRS is {gdf_occurrences_with_climate.crs}. Attempting reproject to EPSG:4326.\")\n",
    "        try:\n",
    "            gdf_occurrences_with_climate = gdf_occurrences_with_climate.to_crs(\"EPSG:4326\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reprojecting occurrences: {e}. Extraction may fail.\"); selected_vars_descriptive = []\n",
    "    \n",
    "    if selected_vars_descriptive:\n",
    "        coords = [(p.x, p.y) for p in gdf_occurrences_with_climate.geometry]\n",
    "        for desc_name in selected_vars_descriptive:\n",
    "            short_key = short_names_map.get(desc_name)\n",
    "            if not short_key or short_key not in output_cropped_rasters or output_cropped_rasters[short_key]['data'] is None:\n",
    "                print(f\"  Skipping extraction for '{desc_name}': Data not found.\"); gdf_occurrences_with_climate[desc_name] = np.nan; continue\n",
    "            raster_data, raster_transform = output_cropped_rasters[short_key]['data'], output_cropped_rasters[short_key]['transform']\n",
    "            print(f\"  Extracting values for: {desc_name} ({short_key})\")\n",
    "            try:\n",
    "                raster_data_for_sampling = raster_data[np.newaxis, :, :] if raster_data.ndim == 2 else raster_data\n",
    "                with rasterio.MemoryFile() as memfile:\n",
    "                    with memfile.open(driver='GTiff', height=raster_data_for_sampling.shape[1], width=raster_data_for_sampling.shape[2], \n",
    "                                      count=raster_data_for_sampling.shape[0], dtype=str(raster_data_for_sampling.dtype), \n",
    "                                      crs='EPSG:4326', transform=raster_transform) as dataset:\n",
    "                        dataset.write(raster_data_for_sampling)\n",
    "                    with memfile.open() as dataset_to_sample:\n",
    "                        extracted_values = [val[0] if val is not None and len(val)>0 else np.nan for val in dataset_to_sample.sample(coords, masked=True)]\n",
    "                gdf_occurrences_with_climate[desc_name] = extracted_values\n",
    "                print(f\"    Successfully extracted. Min: {np.nanmin(extracted_values):.2f}, Max: {np.nanmax(extracted_values):.2f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error extracting values for {desc_name}: {e}\"); gdf_occurrences_with_climate[desc_name] = np.nan\n",
    "\n",
    "        print(\"\\nClimate data extraction complete.\")\n",
    "        valid_climate_cols = [col for col in selected_vars_descriptive if col in gdf_occurrences_with_climate.columns]\n",
    "        print(\"Sample of occurrences with extracted climate data:\")\n",
    "        print(gdf_occurrences_with_climate[['simpSciName'] + valid_climate_cols].head())\n",
    "        \n",
    "        # --- Part 2a: Overall Climate Niche (Box Plots) ---\n",
    "        print(\"\\n--- Part 2a: Overall Climate Niche (All Species Together) ---\")\n",
    "        if valid_climate_cols and not gdf_occurrences_with_climate[valid_climate_cols].empty:\n",
    "            num_vars = len(valid_climate_cols); plots_per_row = 4; num_rows = (num_vars + plots_per_row - 1) // plots_per_row\n",
    "            if num_vars > 0 and num_rows > 0:\n",
    "                plt.figure(figsize=(max(15, plots_per_row * 4), num_rows * 4))\n",
    "                for i, var_name in enumerate(valid_climate_cols):\n",
    "                    plt.subplot(num_rows, plots_per_row, i + 1)\n",
    "                    sns.boxplot(y=gdf_occurrences_with_climate[var_name].dropna())\n",
    "                    unit = variable_units.get(var_name, ''); plt.title(var_name, fontsize=10)\n",
    "                    plt.ylabel(f\"Value ({unit})\" if unit else \"Value\", fontsize=9); plt.xticks([])\n",
    "                    plt.tick_params(axis='y', labelsize=8)\n",
    "                plt.suptitle('Overall Climatic Niche for Amorpha spp.', fontsize=16, y=1.02 if num_rows >1 else 1.05)\n",
    "                plt.tight_layout(rect=[0, 0, 1, 0.98 if num_rows >1 else 0.95]); plt.show()\n",
    "            else: print(\"No valid climate variables to plot for overall niche.\")\n",
    "        else: print(\"No climate data extracted to plot overall niche.\")\n",
    "\n",
    "        # --- Part 2b: Species-Specific Climate Niches (Box Plots) ---\n",
    "        print(\"\\n--- Part 2b: Species-Specific Climate Niches ---\")\n",
    "        if 'simpSciName' in gdf_occurrences_with_climate.columns and valid_climate_cols and not gdf_occurrences_with_climate[valid_climate_cols].empty:\n",
    "            unique_species_in_data = sorted(gdf_occurrences_with_climate['simpSciName'].dropna().unique())\n",
    "            # Ensure palette uses the main species_colors, defaulting if a species isn't in the map\n",
    "            species_specific_palette = {s: species_colors.get(s, '#808080') for s in unique_species_in_data}\n",
    "\n",
    "            for var_name in valid_climate_cols:\n",
    "                if gdf_occurrences_with_climate[var_name].notna().sum() < 1:\n",
    "                    print(f\"Skipping boxplot for {var_name} as it contains all NaN values.\"); continue\n",
    "                plt.figure(figsize=(max(10, len(unique_species_in_data) * 0.8), 6))\n",
    "                sns.boxplot(x='simpSciName', y=var_name, \n",
    "                            data=gdf_occurrences_with_climate.dropna(subset=[var_name]), \n",
    "                            order=unique_species_in_data, # Use sorted list\n",
    "                            palette=species_specific_palette) # MODIFIED: Use consistent species_colors\n",
    "                unit = variable_units.get(var_name, ''); plt.title(f'Climatic Niche for: {var_name}', fontsize=14)\n",
    "                plt.xlabel('Species', fontsize=10); plt.ylabel(f\"Value ({unit})\" if unit else \"Value\", fontsize=10)\n",
    "                plt.xticks(rotation=45, ha='right', fontsize=8); plt.yticks(fontsize=9)\n",
    "                plt.tight_layout(); plt.show()\n",
    "        else: print(\"Insufficient data for species-specific boxplots.\")\n",
    "\n",
    "        # --- Part 2c: PCA ---\n",
    "        print(\"\\n--- Part 2c: Principal Component Analysis (PCA) ---\")\n",
    "        pca_data = gdf_occurrences_with_climate[valid_climate_cols].dropna()\n",
    "        if pca_data.shape[0] < 2 or pca_data.shape[1] < 2:\n",
    "            print(f\"Skipping PCA: Not enough data after NaN removal (Samples: {pca_data.shape[0]}, Features: {pca_data.shape[1]})\")\n",
    "        else:\n",
    "            print(f\"Performing PCA on {pca_data.shape[0]} samples and {pca_data.shape[1]} climate variables.\")\n",
    "            scaler = StandardScaler(); scaled_data = scaler.fit_transform(pca_data)\n",
    "            pca = PCA(n_components=None); pca_transformed_data = pca.fit_transform(scaled_data)\n",
    "            explained_variance_ratio = pca.explained_variance_ratio_\n",
    "            print(f\"\\nExplained variance by each component: {np.round(explained_variance_ratio, 3)}\")\n",
    "            print(f\"Cumulative explained variance: {np.round(np.cumsum(explained_variance_ratio), 3)}\")\n",
    "            print(f\"Variance explained by PC1: {explained_variance_ratio[0]*100:.2f}%\")\n",
    "            if len(explained_variance_ratio) > 1:\n",
    "                print(f\"Variance explained by PC2: {explained_variance_ratio[1]*100:.2f}%\")\n",
    "                print(f\"Variance explained by PC1 & PC2: {(explained_variance_ratio[0] + explained_variance_ratio[1])*100:.2f}%\")\n",
    "            else: print(\"Only one principal component was computed.\")\n",
    "\n",
    "            if pca_transformed_data.shape[1] >= 2:\n",
    "                pca_df = pd.DataFrame(data=pca_transformed_data[:, :2], columns=['PC1', 'PC2'])\n",
    "                pca_df['simpSciName'] = gdf_occurrences_with_climate.loc[pca_data.index, 'simpSciName'].values\n",
    "                \n",
    "                unique_species_in_pca = sorted(pca_df['simpSciName'].unique())\n",
    "                pca_palette = {s: species_colors.get(s, '#808080') for s in unique_species_in_pca}\n",
    "\n",
    "                plt.figure(figsize=(12, 10))\n",
    "                sns.scatterplot(x='PC1', y='PC2', hue='simpSciName', data=pca_df, \n",
    "                                palette=pca_palette, # MODIFIED: Use consistent species_colors\n",
    "                                hue_order=unique_species_in_pca, # Consistent legend order\n",
    "                                s=20, alpha=0.7)\n",
    "                loadings = pca.components_[:2, :].T\n",
    "                pc1_max_abs, pc2_max_abs = 0,0\n",
    "                if not pca_df.empty:\n",
    "                    if 'PC1' in pca_df.columns and pca_df['PC1'].notna().any(): pc1_max_abs = np.abs(pca_df['PC1']).max()\n",
    "                    if 'PC2' in pca_df.columns and pca_df['PC2'].notna().any(): pc2_max_abs = np.abs(pca_df['PC2']).max()\n",
    "                arrow_scale_base = max(pc1_max_abs, pc2_max_abs) if (pc1_max_abs > 0 or pc2_max_abs > 0) else 1.0\n",
    "                arrow_scale = np.sqrt(arrow_scale_base) * 1.2\n",
    "                if pd.isna(arrow_scale) or np.isinf(arrow_scale) : arrow_scale = 1.5\n",
    "                for i, var_name in enumerate(pca_data.columns):\n",
    "                    plt.arrow(0,0,loadings[i,0]*arrow_scale,loadings[i,1]*arrow_scale,color='black',alpha=0.7,head_width=0.08,head_length=0.1,overhang=0.3)\n",
    "                    plt.text(loadings[i,0]*arrow_scale*1.15,loadings[i,1]*arrow_scale*1.15,var_name,color='dimgray',ha='center',va='center',fontsize=9)\n",
    "                plt.xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}%)',fontsize=12); plt.ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}%)',fontsize=12)\n",
    "                plt.title('PCA of Climate Variables for Amorpha Occurrences',fontsize=16); plt.axhline(0,color='grey',ls='--',lw=0.7,alpha=0.5); plt.axvline(0,color='grey',ls='--',lw=0.7,alpha=0.5)\n",
    "                plt.grid(True,ls=':',alpha=0.4); plt.legend(title='Species',bbox_to_anchor=(1.05,1),loc='upper left',fontsize=9,title_fontsize=10)\n",
    "                plt.tight_layout(rect=[0,0,0.85,1]); plt.show()\n",
    "                print(\"\\n--- Loadings for Principal Components ---\")\n",
    "                loadings_df = pd.DataFrame(pca.components_[:2,:].T,columns=['PC1','PC2'],index=pca_data.columns)\n",
    "                print(loadings_df.sort_values(by='PC1',ascending=False,key=abs))\n",
    "                print(\"\\nVariables contributing most to PC1 (absolute value):\"); print(loadings_df['PC1'].abs().sort_values(ascending=False).head())\n",
    "                print(\"\\nVariables contributing most to PC2 (absolute value):\"); print(loadings_df['PC2'].abs().sort_values(ascending=False).head())\n",
    "            elif pca_transformed_data.shape[1] == 1:\n",
    "                print(\"Only one principal component available. Biplot cannot be generated.\"); pca_df = pd.DataFrame(data=pca_transformed_data[:,:1],columns=['PC1'])\n",
    "                pca_df['simpSciName'] = gdf_occurrences_with_climate.loc[pca_data.index,'simpSciName'].values\n",
    "                unique_species_in_pca_pc1 = sorted(pca_df['simpSciName'].unique())\n",
    "                pca_palette_pc1 = {s: species_colors.get(s, '#808080') for s in unique_species_in_pca_pc1}\n",
    "                plt.figure(figsize=(max(10,len(unique_species_in_pca_pc1)*0.6),6))\n",
    "                sns.stripplot(x='simpSciName',y='PC1',data=pca_df,jitter=True,palette=pca_palette_pc1, order=unique_species_in_pca_pc1)\n",
    "                plt.title('PC1 Scores by Species'); plt.ylabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}%)'); plt.xticks(rotation=45,ha='right'); plt.tight_layout(); plt.show()\n",
    "                print(\"\\n--- Loadings for Principal Component 1 ---\"); loadings_df = pd.DataFrame(pca.components_[:1,:].T,columns=['PC1'],index=pca_data.columns)\n",
    "                print(loadings_df.sort_values(by='PC1',ascending=False,key=abs))\n",
    "            else: print(\"No principal components with significant variance found for biplot.\")\n",
    "# This is the end of the main 'else' block\n",
    "print(\"\\n--- All analyses complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14b: Additonal PCA visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 14b: Additonal PCA visualizations ---\n",
    "\n",
    "# Placeholder for pca_df if not defined\n",
    "if 'pca_df' not in globals():\n",
    "    print(\"Warning: 'pca_df' not found. Creating a placeholder for visualization structure.\")\n",
    "    # Try to get species names if gdf_occurrences_with_climate exists\n",
    "    species_names_for_placeholder = ['Species A', 'Species B']\n",
    "    if 'gdf_occurrences_with_climate' in globals() and 'simpSciName' in gdf_occurrences_with_climate.columns:\n",
    "        unique_spp = gdf_occurrences_with_climate['simpSciName'].dropna().unique()\n",
    "        if len(unique_spp) > 0:\n",
    "            species_names_for_placeholder = np.random.choice(unique_spp, size=100, replace=True)\n",
    "        else: # if no species, use default placeholder names\n",
    "            species_names_for_placeholder = np.random.choice(species_names_for_placeholder, size=100, replace=True)\n",
    "\n",
    "\n",
    "    pca_df = pd.DataFrame({\n",
    "        'PC1': np.random.randn(100), \n",
    "        'PC2': np.random.randn(100),\n",
    "        'simpSciName': species_names_for_placeholder\n",
    "    })\n",
    "\n",
    "# Placeholder for loadings, pca_data, explained_variance_ratio, loadings_df, pca object\n",
    "if 'pca_data' not in globals(): # pca_data has descriptive column names\n",
    "    print(\"Warning: 'pca_data' not found. Creating placeholder.\")\n",
    "    # Try to get selected_vars_descriptive if available\n",
    "    if 'selected_vars_descriptive' in globals() and selected_vars_descriptive:\n",
    "        pca_data_cols = selected_vars_descriptive[:max(2,len(selected_vars_descriptive)//2)] # Take a subset\n",
    "    else:\n",
    "        pca_data_cols = ['BIO1 Ann Mean Temp', 'BIO12 Ann Precip', 'Elevation']\n",
    "    pca_data = pd.DataFrame(np.random.rand(100, len(pca_data_cols)), columns=pca_data_cols)\n",
    "\n",
    "\n",
    "if 'loadings' not in globals() or 'pca' not in globals(): # loadings (numpy array), pca (sklearn object)\n",
    "    print(\"Warning: 'loadings' array or 'pca' object not found. Creating placeholders.\")\n",
    "    _num_vars = len(pca_data.columns)\n",
    "    loadings = np.random.rand(_num_vars, 2) # Dummy loadings for PC1, PC2\n",
    "    class PlaceholderPCA:\n",
    "        explained_variance_ratio_ = np.array([0.4, 0.3] + [0.1] * (_num_vars-2) if _num_vars > 2 else [0.4,0.3])\n",
    "        explained_variance_ratio_ = explained_variance_ratio_[explained_variance_ratio_ > 0]\n",
    "        if len(explained_variance_ratio_) > _num_vars: explained_variance_ratio_ = explained_variance_ratio_[:_num_vars]\n",
    "        if sum(explained_variance_ratio_) > 1 : explained_variance_ratio_ = explained_variance_ratio_/sum(explained_variance_ratio_)\n",
    "\n",
    "\n",
    "    pca = PlaceholderPCA()\n",
    "\n",
    "if 'explained_variance_ratio' not in globals(): # Should be pca.explained_variance_ratio_\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "\n",
    "if 'loadings_df' not in globals():\n",
    "    print(\"Warning: 'loadings_df' not found. Creating placeholder.\")\n",
    "    loadings_df = pd.DataFrame(loadings[:,:2] if loadings.shape[1] >=2 else loadings, \n",
    "                               columns=['PC1', 'PC2'][:loadings.shape[1]], \n",
    "                               index=pca_data.columns)\n",
    "# --- End Prerequisite Checks ---\n",
    "\n",
    "\n",
    "# --- Visualization 1: 2D Density Plot (KDE) of PC Scores ---\n",
    "print(\"\\n--- PCA Viz 1: 2D Density Plot (KDE) ---\")\n",
    "if 'pca_df' in globals() and not pca_df.empty and 'PC1' in pca_df.columns and 'PC2' in pca_df.columns \\\n",
    "   and len(pca_df) > 2: # KDE needs a few points\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.kdeplot(x='PC1', y='PC2', data=pca_df, \n",
    "                fill=True, cmap=\"viridis\", thresh=0.05, alpha=0.7, levels=10)\n",
    "    \n",
    "    if 'loadings' in globals() and 'pca_data' in globals() and loadings.shape[0] == len(pca_data.columns) and loadings.shape[1] >=2:\n",
    "        arrow_scale_base = max(np.abs(pca_df['PC1']).max(), np.abs(pca_df['PC2']).max()) if not pca_df.empty and pca_df['PC1'].notna().any() and pca_df['PC2'].notna().any() else 1.0\n",
    "        arrow_scale = np.sqrt(arrow_scale_base) * 1.2 if arrow_scale_base > 0 else 1.2\n",
    "        if pd.isna(arrow_scale) or np.isinf(arrow_scale) : arrow_scale = 1.5\n",
    "        for i, var_name in enumerate(pca_data.columns):\n",
    "            plt.arrow(0,0,loadings[i,0]*arrow_scale,loadings[i,1]*arrow_scale,color='red',alpha=0.8,head_width=0.08,head_length=0.1,overhang=0.3)\n",
    "            plt.text(loadings[i,0]*arrow_scale*1.15,loadings[i,1]*arrow_scale*1.15,var_name,color='black',ha='center',va='center',fontsize=9, weight='bold')\n",
    "\n",
    "    plt.xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}%)', fontsize=12)\n",
    "    plt.ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}%)', fontsize=12)\n",
    "    plt.title('2D Density of Occurrence Points in PC Space', fontsize=16)\n",
    "    plt.axhline(0, color='grey', linestyle='--', linewidth=0.7, alpha=0.5)\n",
    "    plt.axvline(0, color='grey', linestyle='--', linewidth=0.7, alpha=0.5)\n",
    "    plt.grid(True, linestyle=':', alpha=0.4); plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"Skipping 2D Density Plot: pca_df or required columns/data missing or insufficient.\")\n",
    "\n",
    "\n",
    "# --- Visualization 2: Heatmap of PC Loadings ---\n",
    "print(\"\\n--- PCA Viz 3: Heatmap of Loadings ---\")\n",
    "if 'loadings_df' in globals() and not loadings_df.empty and ('PC1' in loadings_df.columns and 'PC2' in loadings_df.columns):\n",
    "    plt.figure(figsize=(8, max(6, len(loadings_df) * 0.45)))\n",
    "    sns.heatmap(loadings_df[['PC1', 'PC2']], annot=True, cmap=\"coolwarm\", center=0, fmt=\".2f\", linewidths=.5, cbar_kws={'label': 'Loading Value'})\n",
    "    plt.title('Heatmap of PCA Loadings for PC1 & PC2', fontsize=14)\n",
    "    plt.ylabel('Original Climate Variables', fontsize=10); plt.xlabel('Principal Components', fontsize=10)\n",
    "    plt.xticks(rotation=0); plt.yticks(rotation=0, fontsize=9); plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"Skipping Loadings Heatmap: loadings_df or required PCs not available.\")\n",
    "\n",
    "\n",
    "# --- Visualization 3: Scree Plot (Explained Variance) ---\n",
    "print(\"\\n--- PCA Viz 4: Scree Plot ---\")\n",
    "if 'pca' in globals() and hasattr(pca, 'explained_variance_ratio_') and len(pca.explained_variance_ratio_) > 0:\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "    num_components = len(explained_variance)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(1, num_components + 1), explained_variance, alpha=0.7, align='center', label='Individual explained variance', color='skyblue')\n",
    "    plt.step(range(1, num_components + 1), cumulative_explained_variance, where='mid', label='Cumulative explained variance', color='red', linestyle='--')\n",
    "    plt.ylabel('Explained Variance Ratio', fontsize=12); plt.xlabel('Principal Component Index', fontsize=12)\n",
    "    plt.title('Scree Plot - Explained Variance by Principal Component', fontsize=14)\n",
    "    plt.xticks(ticks=range(1, num_components + 1)); plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.legend(loc='best'); plt.grid(True, linestyle=':', alpha=0.6); plt.tight_layout()\n",
    "    for i, cum_var in enumerate(cumulative_explained_variance):\n",
    "        if i < 5 or (i > 0 and cum_var > 0.9 and cumulative_explained_variance[i-1] < 0.9) or i == num_components -1 :\n",
    "            plt.text(i + 1.1, cum_var + 0.02, f\"{cum_var*100:.1f}%\", fontsize=8, color='red')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping Scree Plot: PCA object or explained_variance_ratio_ not available or empty.\")\n",
    "\n",
    "print(\"\\n--- All PCA visualizations attempted ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14c: PCA by species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 14c: PCA by species ---\n",
    "\n",
    "# Placeholder checks (assuming these are defined from your PCA cell)\n",
    "if 'pca_df' not in globals() or pca_df.empty:\n",
    "    print(\"Error: 'pca_df' not found or empty. Cannot generate species-specific KDEs.\")\n",
    "    pca_df = pd.DataFrame({'PC1': [], 'PC2': [], 'simpSciName': []}) \n",
    "if 'loadings' not in globals(): loadings = np.array([[0,0],[0,0]]) \n",
    "if 'pca_data' not in globals(): pca_data = pd.DataFrame(columns=['Var1', 'Var2']) \n",
    "if 'explained_variance_ratio' not in globals(): explained_variance_ratio = np.array([0.5, 0.5])\n",
    "if 'species_colors' not in globals(): species_colors = {} \n",
    "\n",
    "# --- Static Individual 2D KDE Plots per Species with FIXED AXES ---\n",
    "print(\"\\n--- PCA Viz: Individual 2D Density Plots (KDE) per Species (Fixed Axes) ---\")\n",
    "\n",
    "if 'pca_df' in globals() and not pca_df.empty and \\\n",
    "   'PC1' in pca_df.columns and 'PC2' in pca_df.columns and \\\n",
    "   'simpSciName' in pca_df.columns and len(pca_df) > 2 :\n",
    "\n",
    "    unique_species = sorted(pca_df['simpSciName'].dropna().unique())\n",
    "\n",
    "    if not unique_species:\n",
    "        print(\"No unique species found in pca_df to plot.\")\n",
    "    else:\n",
    "        # --- Determine overall PC score ranges for fixed axes ---\n",
    "        pc1_min_overall = pca_df['PC1'].min()\n",
    "        pc1_max_overall = pca_df['PC1'].max()\n",
    "        pc2_min_overall = pca_df['PC2'].min()\n",
    "        pc2_max_overall = pca_df['PC2'].max()\n",
    "\n",
    "        # Add a small buffer (e.g., 5-10% of the range)\n",
    "        pc1_range = pc1_max_overall - pc1_min_overall\n",
    "        pc2_range = pc2_max_overall - pc2_min_overall\n",
    "        \n",
    "        # Handle case where range is zero (e.g. all points at the same PC value)\n",
    "        buffer_pc1 = pc1_range * 0.05 if pc1_range > 1e-6 else 0.5 \n",
    "        buffer_pc2 = pc2_range * 0.05 if pc2_range > 1e-6 else 0.5\n",
    "        \n",
    "        fixed_xlim = (pc1_min_overall - buffer_pc1, pc1_max_overall + buffer_pc1)\n",
    "        fixed_ylim = (pc2_min_overall - buffer_pc2, pc2_max_overall + buffer_pc2)\n",
    "        print(f\"  Using fixed axes for all species plots: Xlim={fixed_xlim}, Ylim={fixed_ylim}\")\n",
    "\n",
    "        # Determine a consistent arrow scale based on overall PC score ranges\n",
    "        overall_arrow_scale_base = max(abs(fixed_xlim[0]), abs(fixed_xlim[1]), abs(fixed_ylim[0]), abs(fixed_ylim[1]))\n",
    "        overall_arrow_scale = np.sqrt(overall_arrow_scale_base) * 1.0 if overall_arrow_scale_base > 0 else 1.0\n",
    "        if pd.isna(overall_arrow_scale) or np.isinf(overall_arrow_scale) or overall_arrow_scale == 0: overall_arrow_scale = 1.0\n",
    "\n",
    "\n",
    "        for species_name in unique_species:\n",
    "            species_data_pca = pca_df[pca_df['simpSciName'] == species_name]\n",
    "\n",
    "            if len(species_data_pca) < 3: \n",
    "                print(f\"  Skipping KDE for '{species_name}': Insufficient data points ({len(species_data_pca)}).\")\n",
    "                continue\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            species_color = species_colors.get(species_name, sns.color_palette()[0])\n",
    "\n",
    "            sns.kdeplot(x='PC1', y='PC2', data=species_data_pca, \n",
    "                        fill=True, color=species_color, alpha=0.6,\n",
    "                        thresh=0.05, levels=8)\n",
    "            \n",
    "            plt.scatter(species_data_pca['PC1'], species_data_pca['PC2'], \n",
    "                        color=species_color, s=10, alpha=0.3)\n",
    "\n",
    "            # Add Loading arrows (using overall_arrow_scale for consistency)\n",
    "            if 'loadings' in globals() and 'pca_data' in globals() and \\\n",
    "               loadings.shape[0] == len(pca_data.columns) and loadings.shape[1] >=2 :\n",
    "                for i, var_name in enumerate(pca_data.columns):\n",
    "                    plt.arrow(0,0,loadings[i,0]*overall_arrow_scale,loadings[i,1]*overall_arrow_scale,color='black',alpha=0.7,head_width=0.05,head_length=0.08,overhang=0.3,zorder=5)\n",
    "                    plt.text(loadings[i,0]*overall_arrow_scale*1.15,loadings[i,1]*overall_arrow_scale*1.15,var_name,color='dimgray',ha='center',va='center',fontsize=8,zorder=5,\n",
    "                             path_effects=[withStroke(linewidth=0.5, foreground='white')])\n",
    "\n",
    "            # Apply fixed axes limits\n",
    "            plt.xlim(fixed_xlim)\n",
    "            plt.ylim(fixed_ylim)\n",
    "\n",
    "            plt.xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}%)', fontsize=12)\n",
    "            plt.ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}%)', fontsize=12)\n",
    "            plt.title(f'2D Density (KDE) in PC Space for: {species_name}', fontsize=14)\n",
    "            plt.axhline(0, color='grey', linestyle='--', linewidth=0.7, alpha=0.5)\n",
    "            plt.axvline(0, color='grey', linestyle='--', linewidth=0.7, alpha=0.5)\n",
    "            plt.grid(True, linestyle=':', alpha=0.4)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"Skipping Individual Species KDE Plots: pca_df or required columns/data missing or insufficient.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
